{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f184d71-9d3a-4cdb-adb0-0b208ce152c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "# Suppress warnings globally\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ed41af-c9a2-4b54-89b2-11b5adb0f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRWithVarianceThreshold(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Custom SVR estimator with integrated PCA preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, variance_threshold=0.95, kernel='rbf', C=1, \n",
    "                 gamma='scale', epsilon=0.1, max_iter=-1):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Apply power scaling and PCA with the specified variance threshold\n",
    "        self.scaler_ = PowerTransformer()\n",
    "        X_scaled = self.scaler_.fit_transform(X)\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca.fit(X_scaled)\n",
    "        cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum_variance >= self.variance_threshold) + 1\n",
    "        self.pca_ = PCA(n_components=n_components)\n",
    "        X_pca = self.pca_.fit_transform(X_scaled)\n",
    "        \n",
    "        # **FIX: Store the sign correction for consistency**\n",
    "        self.pca_signs_ = np.sign(self.pca_.components_[0, :n_components])\n",
    "        X_pca *= self.pca_signs_  # Apply sign correction\n",
    "        \n",
    "        # Create and fit the SVR model\n",
    "        base_svr = SVR(kernel=self.kernel, C=self.C, \n",
    "                      gamma=self.gamma, epsilon=self.epsilon, \n",
    "                      max_iter=self.max_iter)\n",
    "        self.model_ = MultiOutputRegressor(base_svr)\n",
    "        self.model_.fit(X_pca, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        X_pca = self.pca_.transform(X_scaled)\n",
    "        X_pca *= self.pca_signs_  # **FIX: Apply same sign correction**\n",
    "        return self.model_.predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658a0644-c0af-4a64-938f-256dc771f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondStageSVRTrainer:\n",
    "    \"\"\"\n",
    "    A class for training second-stage SVR using predictions from a pre-trained first-stage model\n",
    "    combined with extra features (MeanNIQE, MeanSSIM).\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        \"\"\"Initialize the second-stage trainer.\"\"\"\n",
    "        self.first_stage_model = None\n",
    "        self.model = None  # Second-stage model\n",
    "        self.scaler = StandardScaler()\n",
    "        self.best_params = None\n",
    "        self.random_state = random_state\n",
    "        self.label_columns = ['TSV', 'B', 'SR', 'S', 'U', 'O']\n",
    "        self.logistic_model = \"4pl\"\n",
    "        \n",
    "        # Data storage\n",
    "        self.features_df = None\n",
    "        self.labels_df = None\n",
    "        self.extra_features_df = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.extra_train = None\n",
    "        self.extra_test = None\n",
    "\n",
    "    def load_first_stage_model(self, filepath):\n",
    "        \"\"\"Load the pre-trained first-stage model from pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_package = pickle.load(f)\n",
    "            \n",
    "            self.first_stage_model = model_package['model']\n",
    "            print(f\"\\nFirst-stage model loaded successfully from: {filepath}\")\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: First-stage model file not found at {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading first-stage model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_data(self, features_file, labels_file, extra_features_file):\n",
    "        \"\"\"Load features, labels, and extra features from CSV files.\"\"\"\n",
    "        print(\"Loading data for second-stage training...\")\n",
    "        \n",
    "        # Load all files\n",
    "        self.features_df = pd.read_csv(features_file)\n",
    "        self.labels_df = pd.read_csv(labels_file)\n",
    "        self.extra_features_df = pd.read_csv(extra_features_file)\n",
    "        \n",
    "        print(f\"Features shape: {self.features_df.shape}\")\n",
    "        print(f\"Labels shape: {self.labels_df.shape}\")\n",
    "        print(f\"Extra features shape: {self.extra_features_df.shape}\")\n",
    "        \n",
    "        # Merge dataframes on videoname\n",
    "        merged_df = pd.merge(self.features_df, self.labels_df, on='videoname')\n",
    "        merged_all = pd.merge(merged_df, self.extra_features_df, on='videoname')\n",
    "        print(f\"Merged data shape: {merged_all.shape}\")\n",
    "        \n",
    "        # Extract features (all columns except videoname from features file)\n",
    "        feature_columns = [col for col in self.features_df.columns if col != 'videoname']\n",
    "        self.X = merged_all[feature_columns].values\n",
    "        \n",
    "        # Extract labels\n",
    "        self.y = merged_all[self.label_columns].values\n",
    "        \n",
    "        # Store extra features separately\n",
    "        self.extra_features_data = merged_all[['MeanNIQE', 'MeanSSIM']].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Final features shape: {self.X.shape}\")\n",
    "        print(f\"Final labels shape: {self.y.shape}\")\n",
    "        print(f\"Extra features shape: {self.extra_features_data.shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=None):\n",
    "        \"\"\"Split data into train and test sets.\"\"\"\n",
    "        if random_state is None:\n",
    "            random_state = self.random_state\n",
    "            \n",
    "        print(f\"\\nSplitting data into train/test sets (test_size={test_size}, random_state={random_state})...\")\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test, self.extra_train, self.extra_test = train_test_split(\n",
    "            self.X, self.y, self.extra_features_data, \n",
    "            test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Test data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "        print(f\"Extra features train/test shapes: {self.extra_train.shape}, {self.extra_test.shape}\")\n",
    "\n",
    "    def prepare_second_stage_features(self, X, extra_features_data, feature_choice='niqe_ssim'):\n",
    "        \"\"\"Prepare features for second-stage training by combining first-stage predictions with extra features.\"\"\"\n",
    "        if self.first_stage_model is None:\n",
    "            raise ValueError(\"First-stage model not loaded. Run load_first_stage_model() first.\")\n",
    "        \n",
    "        # Select extra features based on choice\n",
    "        if feature_choice == 'niqe':\n",
    "            extra_cols = ['MeanNIQE']\n",
    "        elif feature_choice == 'ssim':\n",
    "            extra_cols = ['MeanSSIM']\n",
    "        elif feature_choice == 'niqe_ssim':\n",
    "            extra_cols = ['MeanNIQE', 'MeanSSIM']\n",
    "        else:\n",
    "            raise ValueError(\"feature_choice must be one of ['niqe', 'ssim', 'niqe_ssim']\")\n",
    "        \n",
    "        # Get selected extra features\n",
    "        extra_selected = extra_features_data[extra_cols].values\n",
    "        \n",
    "        # Get first-stage predictions\n",
    "        first_stage_pred = self.first_stage_model.predict(X)\n",
    "        \n",
    "        # Combine first-stage predictions with extra features\n",
    "        combined_features = np.hstack((first_stage_pred, extra_selected))\n",
    "        \n",
    "        return combined_features\n",
    "\n",
    "    def tune_hyperparameters(self, feature_choice='niqe_ssim', cv=5):\n",
    "        \"\"\"Perform hyperparameter tuning for second-stage SVR.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not split yet. Run split_data() first.\")\n",
    "        if self.first_stage_model is None:\n",
    "            raise ValueError(\"First-stage model not loaded. Run load_first_stage_model() first.\")\n",
    "            \n",
    "        print(f\"\\nTuning hyperparameters for second-stage SVR using features: {feature_choice}\")\n",
    "        print(f\"Cross-validation folds: {cv}\")\n",
    "        \n",
    "        # Prepare training features for second stage\n",
    "        combined_train_features = self.prepare_second_stage_features(\n",
    "            self.X_train, self.extra_train, feature_choice\n",
    "        )\n",
    "        \n",
    "        # Scale the combined features\n",
    "        combined_train_scaled = self.scaler.fit_transform(combined_train_features)\n",
    "        \n",
    "        print(f\"Combined training features shape: {combined_train_scaled.shape}\")\n",
    "        \n",
    "        # Define parameter grid for second-stage SVR\n",
    "        param_grid = {\n",
    "            # 'estimator__kernel': ['rbf', 'linear'],\n",
    "            'estimator__kernel': ['rbf'],\n",
    "            # 'estimator__C': [0.1, 1, 10, 100],\n",
    "            'estimator__C': [1],\n",
    "            # 'estimator__gamma': ['scale', 'auto'],\n",
    "            'estimator__gamma': ['scale'],\n",
    "            # 'estimator__epsilon': [0.01, 0.1],\n",
    "            'estimator__epsilon': [0.1],\n",
    "            # 'estimator__max_iter': [100, 500, 1000, -1]\n",
    "            'estimator__max_iter': [100, 500, -1]\n",
    "        }\n",
    "        \n",
    "        print(f\"Parameter combinations to test: {np.prod([len(values) for values in param_grid.values()])}\")\n",
    "        \n",
    "        # Create base SVR and MultiOutputRegressor\n",
    "        base_svr = SVR()\n",
    "        multi_output_svr = MultiOutputRegressor(base_svr)\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=multi_output_svr,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Starting grid search for second-stage SVR...\")\n",
    "        grid_search.fit(combined_train_scaled, self.y_train)\n",
    "        \n",
    "        # Store best parameters and model\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.model = grid_search.best_estimator_\n",
    "        \n",
    "        print(\"Second-stage hyperparameter tuning completed!\")\n",
    "        self.print_best_parameters()\n",
    "\n",
    "    def print_best_parameters(self):\n",
    "        \"\"\"Print the best hyperparameters found by GridSearchCV.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BEST SECOND-STAGE HYPERPARAMETERS:\")\n",
    "        print(\"=\"*50)\n",
    "        if self.best_params:\n",
    "            for param, value in self.best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "        else:\n",
    "            print(\"No hyperparameters found. Run tune_hyperparameters() first.\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def _logistic_4pl(self, x, A, D, C, B):\n",
    "        \"\"\"4-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / (1 + (x / C) ** B)\n",
    "    \n",
    "    def _logistic_5pl(self, x, A, D, C, B, G):\n",
    "        \"\"\"5-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / ((1 + (x / C) ** B) ** G)\n",
    "        \n",
    "    def _logistic_fit_and_map(self, y_pred: np.ndarray, y_true: np.ndarray, model: str = None):\n",
    "        \"\"\"Fit 4PL/5PL logistic function and return mapped predictions and metrics.\"\"\"\n",
    "        model = (model or self.logistic_model).lower()\n",
    "        x = np.asarray(y_pred).ravel()\n",
    "        y = np.asarray(y_true).ravel()\n",
    "        if model == \"4pl\":\n",
    "            func = self._logistic_4pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0]\n",
    "        else:\n",
    "            func = self._logistic_5pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0, 1.0]\n",
    "        popt, _ = curve_fit(func, x, y, p0=beta0, maxfev=20000)\n",
    "        z = func(x, *popt)\n",
    "        plcc_fitted, _ = pearsonr(z, y)\n",
    "        spearman_fitted, _ = spearmanr(z, y)\n",
    "        rmse_fitted = float(np.sqrt(np.mean((z - y) ** 2)))\n",
    "        return z, popt, plcc_fitted, spearman_fitted, rmse_fitted\n",
    "        \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate performance metrics.\"\"\"\n",
    "        # Remove NaN values\n",
    "        mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "        y_true_clean = y_true[mask]\n",
    "        y_pred_clean = y_pred[mask]\n",
    "        \n",
    "        if len(y_true_clean) == 0:\n",
    "            return {'PLCC': np.nan, 'SRCC': np.nan, 'KRCC': np.nan, 'RMSE': np.nan}\n",
    "        \n",
    "        # PLCC with fitted predictions (4PL/5PL)\n",
    "        try:\n",
    "            _, params, plcc_fitted, _, _ = self._logistic_fit_and_map(\n",
    "                y_pred_clean, y_true_clean, model=self.logistic_model\n",
    "            )\n",
    "            plcc = plcc_fitted\n",
    "        except Exception as e:\n",
    "            print(f\"        Warning: Logistic fitting failed ({e}), using original predictions for PLCC\")\n",
    "            plcc, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "        \n",
    "        # Other metrics with original predictions\n",
    "        srcc, _ = spearmanr(y_true_clean, y_pred_clean)\n",
    "        krcc, _ = kendalltau(y_true_clean, y_pred_clean)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "        \n",
    "        return {'PLCC': plcc, 'SRCC': srcc, 'KRCC': krcc, 'RMSE': rmse}\n",
    "\n",
    "    def evaluate_model(self, feature_choice='niqe_ssim'):\n",
    "        \"\"\"Evaluate the second-stage model on test data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Second-stage model not trained. Run tune_hyperparameters() first.\")\n",
    "        if self.X_test is None:\n",
    "            raise ValueError(\"Test data not available. Run split_data() first.\")\n",
    "\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"SECOND-STAGE MODEL PERFORMANCE ON TEST DATA:\")\n",
    "        print(f\"Using features: {feature_choice}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Prepare test features for second stage\n",
    "        combined_test_features = self.prepare_second_stage_features(\n",
    "            self.X_test, self.extra_test, feature_choice\n",
    "        )\n",
    "        combined_test_scaled = self.scaler.transform(combined_test_features)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(combined_test_scaled)\n",
    "        \n",
    "        # Calculate metrics for each output\n",
    "        print(f\"{'Label':<8} {'PLCC':<8} {'SRCC':<8} {'KRCC':<8} {'RMSE':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        overall_plcc = []\n",
    "        overall_srcc = []\n",
    "        overall_krcc = []\n",
    "        overall_rmse = []\n",
    "        \n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            metrics = self._calculate_metrics(self.y_test[:, i], y_pred[:, i])\n",
    "            \n",
    "            print(f\"{label:<8} {metrics['PLCC']:<8.4f} {metrics['SRCC']:<8.4f} \"\n",
    "                  f\"{metrics['KRCC']:<8.4f} {metrics['RMSE']:<8.4f}\")\n",
    "            \n",
    "            # Store for overall calculations\n",
    "            if not np.isnan(metrics['PLCC']): overall_plcc.append(metrics['PLCC'])\n",
    "            if not np.isnan(metrics['SRCC']): overall_srcc.append(metrics['SRCC'])\n",
    "            if not np.isnan(metrics['KRCC']): overall_krcc.append(metrics['KRCC'])\n",
    "            if not np.isnan(metrics['RMSE']): overall_rmse.append(metrics['RMSE'])\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall metrics across all outputs\n",
    "        if overall_plcc:\n",
    "            print(f\"Mean     {np.mean(overall_plcc):<8.4f} {np.mean(overall_srcc):<8.4f} \"\n",
    "                  f\"{np.mean(overall_krcc):<8.4f} {np.mean(overall_rmse):<8.4f}\")\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the second-stage model to a pickle file.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No second-stage model to save. Train the model first.\")\n",
    "            \n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'best_params': self.best_params,\n",
    "            'label_columns': self.label_columns,\n",
    "            'logistic_model': self.logistic_model,\n",
    "            'random_state': self.random_state,\n",
    "            'save_timestamp': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_package, f)\n",
    "            \n",
    "            print(f\"\\nSecond-stage model saved successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            print(f\"File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving second-stage model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a previously saved second-stage model from pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_package = pickle.load(f)\n",
    "            \n",
    "            self.model = model_package['model']\n",
    "            self.scaler = model_package['scaler']\n",
    "            self.best_params = model_package['best_params']\n",
    "            self.label_columns = model_package.get('label_columns', ['TSV', 'B', 'SR', 'S', 'U', 'O'])\n",
    "            self.logistic_model = model_package.get('logistic_model', '4pl')\n",
    "            self.random_state = model_package.get('random_state', 42)\n",
    "            \n",
    "            print(f\"\\nSecond-stage model loaded successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            print(f\"Random state: {self.random_state}\")\n",
    "            print(f\"Saved on: {model_package.get('save_timestamp', 'Unknown')}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Second-stage model file not found at {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading second-stage model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def run_complete_pipeline(\n",
    "        self,\n",
    "        first_stage_model_path,\n",
    "        features_file,\n",
    "        labels_file,\n",
    "        extra_features_file,\n",
    "        feature_choice='niqe_ssim',\n",
    "        cv=5,\n",
    "        test_size=0.2,\n",
    "        save_model_path=None,\n",
    "        random_state=42\n",
    "    ):\n",
    "        \"\"\"Run the complete second-stage training pipeline.\"\"\"\n",
    "        print(\"Running Second-Stage SVR Training Pipeline...\")\n",
    "        print(f\"First-stage model: {first_stage_model_path}\")\n",
    "        print(f\"Features: {features_file}\")\n",
    "        print(f\"Labels: {labels_file}\")\n",
    "        print(f\"Extra features: {extra_features_file}\")\n",
    "        print(f\"Feature choice: {feature_choice}\")\n",
    "        print(f\"Random state: {random_state}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Load first-stage model\n",
    "        self.load_first_stage_model(first_stage_model_path)\n",
    "        \n",
    "        # Load and split data\n",
    "        self.load_data(features_file, labels_file, extra_features_file)\n",
    "        self.split_data(test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Train second-stage model\n",
    "        self.tune_hyperparameters(feature_choice=feature_choice, cv=cv)\n",
    "        \n",
    "        # Evaluate second-stage model\n",
    "        self.evaluate_model(feature_choice=feature_choice)\n",
    "        \n",
    "        # Save model if path provided\n",
    "        if save_model_path:\n",
    "            self.save_model(save_model_path)\n",
    "        \n",
    "        print(\"\\nSecond-stage pipeline completed successfully!\")\n",
    "\n",
    "    def predict(self, new_features, extra_features_data, feature_choice='niqe_ssim'):\n",
    "        \"\"\"Make predictions using the trained second-stage model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Second-stage model not trained. Run the pipeline first or load a saved model.\")\n",
    "        if self.first_stage_model is None:\n",
    "            raise ValueError(\"First-stage model not loaded. Run load_first_stage_model() first.\")\n",
    "        \n",
    "        # Prepare features for second stage\n",
    "        combined_features = self.prepare_second_stage_features(\n",
    "            new_features, extra_features_data, feature_choice\n",
    "        )\n",
    "        combined_scaled = self.scaler.transform(combined_features)\n",
    "        \n",
    "        return self.model.predict(combined_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74d793a-f73e-4dff-8687-082b2ab666f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Second-Stage SVR Training Pipeline...\n",
      "First-stage model: .\\trained_models\\svr_pca_model.pkl\n",
      "Features: .\\dataset\\cleaned-svd-features.csv\n",
      "Labels: .\\dataset\\cleaned-mos.csv\n",
      "Extra features: .\\dataset\\cleaned-extra-features.csv\n",
      "Feature choice: ssim\n",
      "Random state: 42\n",
      "============================================================\n",
      "\n",
      "First-stage model loaded successfully from: .\\trained_models\\svr_pca_model.pkl\n",
      "Loading data for second-stage training...\n",
      "Features shape: (1000, 1153)\n",
      "Labels shape: (1000, 7)\n",
      "Extra features shape: (1000, 3)\n",
      "Merged data shape: (1000, 1161)\n",
      "Final features shape: (1000, 1152)\n",
      "Final labels shape: (1000, 6)\n",
      "Extra features shape: (1000, 2)\n",
      "Data loading completed successfully!\n",
      "\n",
      "Splitting data into train/test sets (test_size=0.2, random_state=42)...\n",
      "Training data shape: X=(800, 1152), y=(800, 6)\n",
      "Test data shape: X=(200, 1152), y=(200, 6)\n",
      "Extra features train/test shapes: (800, 2), (200, 2)\n",
      "\n",
      "Tuning hyperparameters for second-stage SVR using features: ssim\n",
      "Cross-validation folds: 5\n",
      "Combined training features shape: (800, 7)\n",
      "Parameter combinations to test: 3\n",
      "Starting grid search for second-stage SVR...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Second-stage hyperparameter tuning completed!\n",
      "\n",
      "==================================================\n",
      "BEST SECOND-STAGE HYPERPARAMETERS:\n",
      "==================================================\n",
      "estimator__C: 1\n",
      "estimator__epsilon: 0.1\n",
      "estimator__gamma: scale\n",
      "estimator__kernel: rbf\n",
      "estimator__max_iter: -1\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "SECOND-STAGE MODEL PERFORMANCE ON TEST DATA:\n",
      "Using features: ssim\n",
      "============================================================\n",
      "Label    PLCC     SRCC     KRCC     RMSE    \n",
      "--------------------------------------------------\n",
      "TSV      0.8764   0.8530   0.6685   0.3218  \n",
      "B        0.7422   0.7157   0.5245   0.5548  \n",
      "SR       0.7569   0.7589   0.5659   0.4538  \n",
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "S        0.8895   0.8802   0.6894   0.3625  \n",
      "U        0.8570   0.8543   0.6622   0.4089  \n",
      "O        0.8815   0.8718   0.6823   0.5151  \n",
      "--------------------------------------------------\n",
      "Mean     0.8339   0.8223   0.6321   0.4361  \n",
      "\n",
      "Second-stage model saved successfully!\n",
      "Filepath: D:\\Vishal\\Studies\\Sem_8\\FYP\\no-reference-colonoscopic-vqa\\objective-1\\regressors-4-niqe-ssim\\svr-pooling\\trained_models\\second_stage_svr_model.pkl\n",
      "File size: 0.17 MB\n",
      "\n",
      "Second-stage pipeline completed successfully!\n",
      "Second-stage training completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    second_stage_trainer = SecondStageSVRTrainer(random_state=42)\n",
    "    \n",
    "    # Run complete pipeline\n",
    "    second_stage_trainer.run_complete_pipeline(\n",
    "        first_stage_model_path=r\".\\trained_models\\svr_pca_model.pkl\",\n",
    "        features_file=r\".\\dataset\\cleaned-svd-features.csv\", \n",
    "        labels_file=r\".\\dataset\\cleaned-mos.csv\",\n",
    "        extra_features_file=r\".\\dataset\\cleaned-extra-features.csv\",\n",
    "        feature_choice='niqe_ssim',  # Options: 'niqe', 'ssim', 'niqe_ssim'\n",
    "        cv=5,\n",
    "        test_size=0.2,\n",
    "        save_model_path=r\".\\trained_models\\second_stage_svr_model.pkl\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Second-stage training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0943766-67f2-4861-94cc-c7888354a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Second-Stage SVR Training Pipeline...\n",
      "First-stage model: .\\trained_models\\svr_pca_model.pkl\n",
      "Features: .\\dataset\\cleaned-svd-features.csv\n",
      "Labels: .\\dataset\\cleaned-mos.csv\n",
      "Extra features: .\\dataset\\cleaned-extra-features.csv\n",
      "Feature choice: niqe_ssim\n",
      "Random state: 42\n",
      "============================================================\n",
      "\n",
      "First-stage model loaded successfully from: .\\trained_models\\svr_pca_model.pkl\n",
      "Loading data for second-stage training...\n",
      "Features shape: (1000, 1153)\n",
      "Labels shape: (1000, 7)\n",
      "Extra features shape: (1000, 3)\n",
      "Merged data shape: (1000, 1161)\n",
      "Final features shape: (1000, 1152)\n",
      "Final labels shape: (1000, 6)\n",
      "Extra features shape: (1000, 2)\n",
      "Data loading completed successfully!\n",
      "\n",
      "Splitting data into train/test sets (test_size=0.2, random_state=42)...\n",
      "Training data shape: X=(800, 1152), y=(800, 6)\n",
      "Test data shape: X=(200, 1152), y=(200, 6)\n",
      "Extra features train/test shapes: (800, 2), (200, 2)\n",
      "\n",
      "Tuning hyperparameters for second-stage SVR using features: niqe_ssim\n",
      "Cross-validation folds: 5\n",
      "Combined training features shape: (800, 8)\n",
      "Parameter combinations to test: 3\n",
      "Starting grid search for second-stage SVR...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Second-stage hyperparameter tuning completed!\n",
      "\n",
      "==================================================\n",
      "BEST SECOND-STAGE HYPERPARAMETERS:\n",
      "==================================================\n",
      "estimator__C: 1\n",
      "estimator__epsilon: 0.1\n",
      "estimator__gamma: scale\n",
      "estimator__kernel: rbf\n",
      "estimator__max_iter: -1\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "SECOND-STAGE MODEL PERFORMANCE ON TEST DATA:\n",
      "Using features: niqe_ssim\n",
      "============================================================\n",
      "Label    PLCC     SRCC     KRCC     RMSE    \n",
      "--------------------------------------------------\n",
      "TSV      0.8777   0.8573   0.6734   0.3203  \n",
      "B        0.7464   0.7172   0.5242   0.5493  \n",
      "SR       0.7517   0.7532   0.5617   0.4590  \n",
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "S        0.8899   0.8806   0.6915   0.3617  \n",
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "U        0.8501   0.8525   0.6589   0.4111  \n",
      "O        0.8829   0.8745   0.6847   0.5121  \n",
      "--------------------------------------------------\n",
      "Mean     0.8331   0.8225   0.6324   0.4356  \n",
      "\n",
      "Second-stage model saved successfully!\n",
      "Filepath: D:\\Vishal\\Studies\\Sem_8\\FYP\\no-reference-colonoscopic-vqa\\objective-1\\regressors-4-niqe-ssim\\svr-pooling\\trained_models\\second_stage_svr_model.pkl\n",
      "File size: 0.20 MB\n",
      "\n",
      "Second-stage pipeline completed successfully!\n",
      "Second-stage training completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "# Suppress warnings globally\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SVRWithVarianceThreshold(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Custom SVR estimator with integrated PCA preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, variance_threshold=0.95, kernel='rbf', C=1, \n",
    "                 gamma='scale', epsilon=0.1, max_iter=-1):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Apply power scaling and PCA with the specified variance threshold\n",
    "        self.scaler_ = PowerTransformer()\n",
    "        X_scaled = self.scaler_.fit_transform(X)\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca.fit(X_scaled)\n",
    "        cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum_variance >= self.variance_threshold) + 1\n",
    "        self.pca_ = PCA(n_components=n_components)\n",
    "        X_pca = self.pca_.fit_transform(X_scaled)\n",
    "        \n",
    "        # **FIX: Store the sign correction for consistency**\n",
    "        self.pca_signs_ = np.sign(self.pca_.components_[0, :n_components])\n",
    "        X_pca *= self.pca_signs_  # Apply sign correction\n",
    "        \n",
    "        # Create and fit the SVR model\n",
    "        base_svr = SVR(kernel=self.kernel, C=self.C, \n",
    "                      gamma=self.gamma, epsilon=self.epsilon, \n",
    "                      max_iter=self.max_iter)\n",
    "        self.model_ = MultiOutputRegressor(base_svr)\n",
    "        self.model_.fit(X_pca, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        X_pca = self.pca_.transform(X_scaled)\n",
    "        X_pca *= self.pca_signs_  # **FIX: Apply same sign correction**\n",
    "        return self.model_.predict(X_pca)\n",
    "\n",
    "class SecondStageSVRTrainer:\n",
    "    \"\"\"\n",
    "    A class for training second-stage SVR using predictions from a pre-trained first-stage model\n",
    "    combined with extra features (MeanNIQE, MeanSSIM).\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        \"\"\"Initialize the second-stage trainer.\"\"\"\n",
    "        self.first_stage_model = None\n",
    "        self.model = None  # Second-stage model\n",
    "        self.scaler = StandardScaler()\n",
    "        self.extra_scaler = StandardScaler()  # NEW: Independent scaler for extra features\n",
    "        self.best_params = None\n",
    "        self.random_state = random_state\n",
    "        self.label_columns = ['TSV', 'B', 'SR', 'S', 'U', 'O']\n",
    "        self.logistic_model = \"4pl\"\n",
    "        \n",
    "        # Data storage\n",
    "        self.features_df = None\n",
    "        self.labels_df = None\n",
    "        self.extra_features_df = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.extra_train = None\n",
    "        self.extra_test = None\n",
    "\n",
    "    def load_first_stage_model(self, filepath):\n",
    "        \"\"\"Load the pre-trained first-stage model from pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_package = pickle.load(f)\n",
    "            \n",
    "            self.first_stage_model = model_package['model']\n",
    "            print(f\"\\nFirst-stage model loaded successfully from: {filepath}\")\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: First-stage model file not found at {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading first-stage model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_data(self, features_file, labels_file, extra_features_file):\n",
    "        \"\"\"Load features, labels, and extra features from CSV files.\"\"\"\n",
    "        print(\"Loading data for second-stage training...\")\n",
    "        \n",
    "        # Load all files\n",
    "        self.features_df = pd.read_csv(features_file)\n",
    "        self.labels_df = pd.read_csv(labels_file)\n",
    "        self.extra_features_df = pd.read_csv(extra_features_file)\n",
    "        \n",
    "        print(f\"Features shape: {self.features_df.shape}\")\n",
    "        print(f\"Labels shape: {self.labels_df.shape}\")\n",
    "        print(f\"Extra features shape: {self.extra_features_df.shape}\")\n",
    "        \n",
    "        # Merge dataframes on videoname\n",
    "        merged_df = pd.merge(self.features_df, self.labels_df, on='videoname')\n",
    "        merged_all = pd.merge(merged_df, self.extra_features_df, on='videoname')\n",
    "        print(f\"Merged data shape: {merged_all.shape}\")\n",
    "        \n",
    "        # Extract features (all columns except videoname from features file)\n",
    "        feature_columns = [col for col in self.features_df.columns if col != 'videoname']\n",
    "        self.X = merged_all[feature_columns].values\n",
    "        \n",
    "        # Extract labels\n",
    "        self.y = merged_all[self.label_columns].values\n",
    "        \n",
    "        # Store extra features separately\n",
    "        self.extra_features_data = merged_all[['MeanNIQE', 'MeanSSIM']].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Final features shape: {self.X.shape}\")\n",
    "        print(f\"Final labels shape: {self.y.shape}\")\n",
    "        print(f\"Extra features shape: {self.extra_features_data.shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=None):\n",
    "        \"\"\"Split data into train and test sets.\"\"\"\n",
    "        if random_state is None:\n",
    "            random_state = self.random_state\n",
    "            \n",
    "        print(f\"\\nSplitting data into train/test sets (test_size={test_size}, random_state={random_state})...\")\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test, self.extra_train, self.extra_test = train_test_split(\n",
    "            self.X, self.y, self.extra_features_data, \n",
    "            test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # NEW: Fit the extra features scaler on training data only\n",
    "        self.extra_scaler.fit(self.extra_train)\n",
    "        \n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Test data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "        print(f\"Extra features train/test shapes: {self.extra_train.shape}, {self.extra_test.shape}\")\n",
    "\n",
    "    def prepare_second_stage_features(self, X, extra_features_data, feature_choice='niqe_ssim'):\n",
    "        \"\"\"Prepare features for second-stage training by combining first-stage predictions with extra features.\"\"\"\n",
    "        if self.first_stage_model is None:\n",
    "            raise ValueError(\"First-stage model not loaded. Run load_first_stage_model() first.\")\n",
    "        \n",
    "        # Select extra features based on choice\n",
    "        if feature_choice == 'niqe':\n",
    "            extra_cols = ['MeanNIQE']\n",
    "        elif feature_choice == 'ssim':\n",
    "            extra_cols = ['MeanSSIM']\n",
    "        elif feature_choice == 'niqe_ssim':\n",
    "            extra_cols = ['MeanNIQE', 'MeanSSIM']\n",
    "        else:\n",
    "            raise ValueError(\"feature_choice must be one of ['niqe', 'ssim', 'niqe_ssim']\")\n",
    "        \n",
    "        # Get selected extra features\n",
    "        extra_selected = extra_features_data[extra_cols].values\n",
    "        \n",
    "        # NEW: Apply independent scaling to extra features\n",
    "        extra_scaled = self.extra_scaler.transform(extra_selected)\n",
    "        \n",
    "        # Get first-stage predictions\n",
    "        first_stage_pred = self.first_stage_model.predict(X)\n",
    "        \n",
    "        # Combine first-stage predictions with scaled extra features\n",
    "        combined_features = np.hstack((first_stage_pred, extra_scaled))\n",
    "        \n",
    "        return combined_features\n",
    "\n",
    "    def tune_hyperparameters(self, feature_choice='niqe_ssim', cv=5):\n",
    "        \"\"\"Perform hyperparameter tuning for second-stage SVR.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not split yet. Run split_data() first.\")\n",
    "        if self.first_stage_model is None:\n",
    "            raise ValueError(\"First-stage model not loaded. Run load_first_stage_model() first.\")\n",
    "            \n",
    "        print(f\"\\nTuning hyperparameters for second-stage SVR using features: {feature_choice}\")\n",
    "        print(f\"Cross-validation folds: {cv}\")\n",
    "        \n",
    "        # Prepare training features for second stage\n",
    "        combined_train_features = self.prepare_second_stage_features(\n",
    "            self.X_train, self.extra_train, feature_choice\n",
    "        )\n",
    "        \n",
    "        # Scale the combined features\n",
    "        combined_train_scaled = self.scaler.fit_transform(combined_train_features)\n",
    "        \n",
    "        print(f\"Combined training features shape: {combined_train_scaled.shape}\")\n",
    "        \n",
    "        # Define parameter grid for second-stage SVR\n",
    "        param_grid = {\n",
    "            # 'estimator__kernel': ['rbf', 'linear'],\n",
    "            'estimator__kernel': ['rbf'],\n",
    "            # 'estimator__C': [0.1, 1, 10, 100],\n",
    "            'estimator__C': [1],\n",
    "            # 'estimator__gamma': ['scale', 'auto'],\n",
    "            'estimator__gamma': ['scale'],\n",
    "            # 'estimator__epsilon': [0.01, 0.1],\n",
    "            'estimator__epsilon': [0.1],\n",
    "            # 'estimator__max_iter': [100, 500, 1000, -1]\n",
    "            'estimator__max_iter': [100, 500, -1]\n",
    "        }\n",
    "        \n",
    "        print(f\"Parameter combinations to test: {np.prod([len(values) for values in param_grid.values()])}\")\n",
    "        \n",
    "        # Create base SVR and MultiOutputRegressor\n",
    "        base_svr = SVR()\n",
    "        multi_output_svr = MultiOutputRegressor(base_svr)\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=multi_output_svr,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Starting grid search for second-stage SVR...\")\n",
    "        grid_search.fit(combined_train_scaled, self.y_train)\n",
    "        \n",
    "        # Store best parameters and model\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.model = grid_search.best_estimator_\n",
    "        \n",
    "        print(\"Second-stage hyperparameter tuning completed!\")\n",
    "        self.print_best_parameters()\n",
    "\n",
    "    def print_best_parameters(self):\n",
    "        \"\"\"Print the best hyperparameters found by GridSearchCV.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BEST SECOND-STAGE HYPERPARAMETERS:\")\n",
    "        print(\"=\"*50)\n",
    "        if self.best_params:\n",
    "            for param, value in self.best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "        else:\n",
    "            print(\"No hyperparameters found. Run tune_hyperparameters() first.\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    def _logistic_4pl(self, x, A, D, C, B):\n",
    "        \"\"\"4-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / (1 + (x / C) ** B)\n",
    "    \n",
    "    def _logistic_5pl(self, x, A, D, C, B, G):\n",
    "        \"\"\"5-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / ((1 + (x / C) ** B) ** G)\n",
    "        \n",
    "    def _logistic_fit_and_map(self, y_pred: np.ndarray, y_true: np.ndarray, model: str = None):\n",
    "        \"\"\"Fit 4PL/5PL logistic function and return mapped predictions and metrics.\"\"\"\n",
    "        model = (model or self.logistic_model).lower()\n",
    "        x = np.asarray(y_pred).ravel()\n",
    "        y = np.asarray(y_true).ravel()\n",
    "        if model == \"4pl\":\n",
    "            func = self._logistic_4pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0]\n",
    "        else:\n",
    "            func = self._logistic_5pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0, 1.0]\n",
    "        popt, _ = curve_fit(func, x, y, p0=beta0, maxfev=20000)\n",
    "        z = func(x, *popt)\n",
    "        plcc_fitted, _ = pearsonr(z, y)\n",
    "        spearman_fitted, _ = spearmanr(z, y)\n",
    "        rmse_fitted = float(np.sqrt(np.mean((z - y) ** 2)))\n",
    "        return z, popt, plcc_fitted, spearman_fitted, rmse_fitted\n",
    "        \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate performance metrics.\"\"\"\n",
    "        # Remove NaN values\n",
    "        mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "        y_true_clean = y_true[mask]\n",
    "        y_pred_clean = y_pred[mask]\n",
    "        \n",
    "        if len(y_true_clean) == 0:\n",
    "            return {'PLCC': np.nan, 'SRCC': np.nan, 'KRCC': np.nan, 'RMSE': np.nan}\n",
    "        \n",
    "        # PLCC with fitted predictions (4PL/5PL)\n",
    "        try:\n",
    "            _, params, plcc_fitted, _, _ = self._logistic_fit_and_map(\n",
    "                y_pred_clean, y_true_clean, model=self.logistic_model\n",
    "            )\n",
    "            plcc = plcc_fitted\n",
    "        except Exception as e:\n",
    "            print(f\"        Warning: Logistic fitting failed ({e}), using original predictions for PLCC\")\n",
    "            plcc, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "        \n",
    "        # Other metrics with original predictions\n",
    "        srcc, _ = spearmanr(y_true_clean, y_pred_clean)\n",
    "        krcc, _ = kendalltau(y_true_clean, y_pred_clean)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "        \n",
    "        return {'PLCC': plcc, 'SRCC': srcc, 'KRCC': krcc, 'RMSE': rmse}\n",
    "\n",
    "    def evaluate_model(self, feature_choice='niqe_ssim'):\n",
    "        \"\"\"Evaluate the second-stage model on test data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Second-stage model not trained. Run tune_hyperparameters() first.\")\n",
    "        if self.X_test is None:\n",
    "            raise ValueError(\"Test data not available. Run split_data() first.\")\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"SECOND-STAGE MODEL PERFORMANCE ON TEST DATA:\")\n",
    "        print(f\"Using features: {feature_choice}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Prepare test features for second stage\n",
    "        combined_test_features = self.prepare_second_stage_features(\n",
    "            self.X_test, self.extra_test, feature_choice\n",
    "        )\n",
    "        combined_test_scaled = self.scaler.transform(combined_test_features)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(combined_test_scaled)\n",
    "        \n",
    "        # Calculate metrics for each output\n",
    "        print(f\"{'Label':<8} {'PLCC':<8} {'SRCC':<8} {'KRCC':<8} {'RMSE':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        overall_plcc = []\n",
    "        overall_srcc = []\n",
    "        overall_krcc = []\n",
    "        overall_rmse = []\n",
    "        \n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            metrics = self._calculate_metrics(self.y_test[:, i], y_pred[:, i])\n",
    "            \n",
    "            print(f\"{label:<8} {metrics['PLCC']:<8.4f} {metrics['SRCC']:<8.4f} \"\n",
    "                  f\"{metrics['KRCC']:<8.4f} {metrics['RMSE']:<8.4f}\")\n",
    "            \n",
    "            # Store for overall calculations\n",
    "            if not np.isnan(metrics['PLCC']): overall_plcc.append(metrics['PLCC'])\n",
    "            if not np.isnan(metrics['SRCC']): overall_srcc.append(metrics['SRCC'])\n",
    "            if not np.isnan(metrics['KRCC']): overall_krcc.append(metrics['KRCC'])\n",
    "            if not np.isnan(metrics['RMSE']): overall_rmse.append(metrics['RMSE'])\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall metrics across all outputs\n",
    "        if overall_plcc:\n",
    "            print(f\"Mean     {np.mean(overall_plcc):<8.4f} {np.mean(overall_srcc):<8.4f} \"\n",
    "                  f\"{np.mean(overall_krcc):<8.4f} {np.mean(overall_rmse):<8.4f}\")\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the second-stage model to a pickle file.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No second-stage model to save. Train the model first.\")\n",
    "            \n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'extra_scaler': self.extra_scaler,  # NEW: Save the extra features scaler\n",
    "            'best_params': self.best_params,\n",
    "            'label_columns': self.label_columns,\n",
    "            'logistic_model': self.logistic_model,\n",
    "            'random_state': self.random_state,\n",
    "            'save_timestamp': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_package, f)\n",
    "            \n",
    "            print(f\"\\nSecond-stage model saved successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            print(f\"File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving second-stage model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a previously saved second-stage model from pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_package = pickle.load(f)\n",
    "            \n",
    "            self.model = model_package['model']\n",
    "            self.scaler = model_package['scaler']\n",
    "            self.extra_scaler = model_package.get('extra_scaler', StandardScaler())  # NEW: Load extra scaler\n",
    "            self.best_params = model_package['best_params']\n",
    "            self.label_columns = model_package.get('label_columns', ['TSV', 'B', 'SR', 'S', 'U', 'O'])\n",
    "            self.logistic_model = model_package.get('logistic_model', '4pl')\n",
    "            self.random_state = model_package.get('random_state', 42)\n",
    "            \n",
    "            print(f\"\\nSecond-stage model loaded successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            print(f\"Random state: {self.random_state}\")\n",
    "            print(f\"Saved on: {model_package.get('save_timestamp', 'Unknown')}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Second-stage model file not found at {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading second-stage model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def run_complete_pipeline(\n",
    "        self,\n",
    "        first_stage_model_path,\n",
    "        features_file,\n",
    "        labels_file,\n",
    "        extra_features_file,\n",
    "        feature_choice='niqe_ssim',\n",
    "        cv=5,\n",
    "        test_size=0.2,\n",
    "        save_model_path=None,\n",
    "        random_state=42\n",
    "    ):\n",
    "        \"\"\"Run the complete second-stage training pipeline.\"\"\"\n",
    "        print(\"Running Second-Stage SVR Training Pipeline...\")\n",
    "        print(f\"First-stage model: {first_stage_model_path}\")\n",
    "        print(f\"Features: {features_file}\")\n",
    "        print(f\"Labels: {labels_file}\")\n",
    "        print(f\"Extra features: {extra_features_file}\")\n",
    "        print(f\"Feature choice: {feature_choice}\")\n",
    "        print(f\"Random state: {random_state}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Load first-stage model\n",
    "        self.load_first_stage_model(first_stage_model_path)\n",
    "        \n",
    "        # Load and split data\n",
    "        self.load_data(features_file, labels_file, extra_features_file)\n",
    "        self.split_data(test_size=test_size, random_state=random_state)\n",
    "        \n",
    "        # Train second-stage model\n",
    "        self.tune_hyperparameters(feature_choice=feature_choice, cv=cv)\n",
    "        \n",
    "        # Evaluate second-stage model\n",
    "        self.evaluate_model(feature_choice=feature_choice)\n",
    "        \n",
    "        # Save model if path provided\n",
    "        if save_model_path:\n",
    "            self.save_model(save_model_path)\n",
    "        \n",
    "        print(\"\\nSecond-stage pipeline completed successfully!\")\n",
    "\n",
    "    def predict(self, new_features, extra_features_data, feature_choice='niqe_ssim'):\n",
    "        \"\"\"Make predictions using the trained second-stage model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Second-stage model not trained. Run the pipeline first or load a saved model.\")\n",
    "        if self.first_stage_model is None:\n",
    "            raise ValueError(\"First-stage model not loaded. Run load_first_stage_model() first.\")\n",
    "        \n",
    "        # Prepare features for second stage\n",
    "        combined_features = self.prepare_second_stage_features(\n",
    "            new_features, extra_features_data, feature_choice\n",
    "        )\n",
    "        combined_scaled = self.scaler.transform(combined_features)\n",
    "        \n",
    "        return self.model.predict(combined_scaled)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    second_stage_trainer = SecondStageSVRTrainer(random_state=42)\n",
    "    \n",
    "    # Run complete pipeline\n",
    "    second_stage_trainer.run_complete_pipeline(\n",
    "        first_stage_model_path=r\".\\trained_models\\svr_pca_model.pkl\",\n",
    "        features_file=r\".\\dataset\\cleaned-svd-features.csv\", \n",
    "        labels_file=r\".\\dataset\\cleaned-mos.csv\",\n",
    "        extra_features_file=r\".\\dataset\\cleaned-extra-features.csv\",\n",
    "        feature_choice='niqe_ssim',  # Options: 'niqe', 'ssim', 'niqe_ssim'\n",
    "        cv=5,\n",
    "        test_size=0.2,\n",
    "        save_model_path=r\".\\trained_models\\second_stage_svr_model.pkl\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Second-stage training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcf50d-c1cb-46d5-877c-35c6f47e5579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
