{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aaa2a5-969f-41f6-b0e6-ef7e6f62d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "# Suppress warnings globally\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05df7870-ae9a-4106-b844-cc8d42f901c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRWithVarianceThreshold(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Custom SVR estimator with integrated PCA preprocessing and extra features.\"\"\"\n",
    "    \n",
    "    def __init__(self, variance_threshold=0.95, kernel='rbf', C=1, \n",
    "                 gamma='scale', epsilon=0.1, max_iter=-1, extra_features_to_use=None, random_state=None):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        # extra_features_to_use: list of indices [0,1] -> 0: MeanNIQE, 1: MeanSSIM\n",
    "        # None or [] means no extra features\n",
    "        self.extra_features_to_use = extra_features_to_use if extra_features_to_use is not None else []\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Separate main PCA features and extra features\n",
    "        X_main = X[:, :-2]  # All main features (exclude last 2 columns)\n",
    "        X_niqe = X[:, -2]   # MeanNIQE (second to last column)\n",
    "        X_ssim = X[:, -1]   # MeanSSIM (last column)\n",
    "        \n",
    "        # Apply PCA to main features only\n",
    "        self.main_scaler_ = StandardScaler()\n",
    "        X_main_scaled = self.main_scaler_.fit_transform(X_main)\n",
    "        \n",
    "        # Determine PCA components\n",
    "        pca = PCA(random_state=self.random_state)\n",
    "        pca.fit(X_main_scaled)\n",
    "        cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum_variance >= self.variance_threshold) + 1\n",
    "        \n",
    "        # Apply PCA with determined components\n",
    "        self.pca_ = PCA(n_components=n_components, random_state=self.random_state)\n",
    "        X_pca = self.pca_.fit_transform(X_main_scaled)\n",
    "        \n",
    "        # Apply sign correction for consistency\n",
    "        self.pca_signs_ = np.sign(self.pca_.components_[:, 0])\n",
    "        X_pca *= self.pca_signs_\n",
    "        \n",
    "        # Handle extra features based on extra_features_to_use parameter\n",
    "        extra_features_list = []\n",
    "        \n",
    "        if 0 in self.extra_features_to_use:  # Include MeanNIQE\n",
    "            self.niqe_scaler_ = StandardScaler()\n",
    "            X_niqe_scaled = self.niqe_scaler_.fit_transform(X_niqe.reshape(-1, 1))\n",
    "            extra_features_list.append(X_niqe_scaled)\n",
    "        \n",
    "        if 1 in self.extra_features_to_use:  # Include MeanSSIM  \n",
    "            self.ssim_scaler_ = StandardScaler()\n",
    "            X_ssim_scaled = self.ssim_scaler_.fit_transform(X_ssim.reshape(-1, 1))\n",
    "            extra_features_list.append(X_ssim_scaled)\n",
    "        \n",
    "        # Combine PCA features with selected extra features\n",
    "        if extra_features_list:\n",
    "            X_extra_combined = np.hstack(extra_features_list)\n",
    "            X_final = np.hstack([X_pca, X_extra_combined])\n",
    "        else:\n",
    "            X_final = X_pca\n",
    "        \n",
    "        # Train SVR model\n",
    "        base_svr = SVR(kernel=self.kernel, C=self.C, \n",
    "                      gamma=self.gamma, epsilon=self.epsilon, \n",
    "                      max_iter=self.max_iter)\n",
    "        self.model_ = MultiOutputRegressor(base_svr)\n",
    "        self.model_.fit(X_final, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Separate main PCA features and extra features\n",
    "        X_main = X[:, :-2]  # All main features (exclude last 2 columns)  \n",
    "        X_niqe = X[:, -2]   # MeanNIQE (second to last column)\n",
    "        X_ssim = X[:, -1]   # MeanSSIM (last column)\n",
    "        \n",
    "        # Apply PCA to main features\n",
    "        X_main_scaled = self.main_scaler_.transform(X_main)\n",
    "        X_pca = self.pca_.transform(X_main_scaled)\n",
    "        X_pca *= self.pca_signs_  # Apply same sign correction\n",
    "        \n",
    "        # Handle extra features based on extra_features_to_use parameter  \n",
    "        extra_features_list = []\n",
    "        \n",
    "        if 0 in self.extra_features_to_use:  # Include MeanNIQE\n",
    "            X_niqe_scaled = self.niqe_scaler_.transform(X_niqe.reshape(-1, 1))\n",
    "            extra_features_list.append(X_niqe_scaled)\n",
    "            \n",
    "        if 1 in self.extra_features_to_use:  # Include MeanSSIM\n",
    "            X_ssim_scaled = self.ssim_scaler_.transform(X_ssim.reshape(-1, 1))\n",
    "            extra_features_list.append(X_ssim_scaled)\n",
    "        \n",
    "        # Combine PCA features with selected extra features\n",
    "        if extra_features_list:\n",
    "            X_extra_combined = np.hstack(extra_features_list)\n",
    "            X_final = np.hstack([X_pca, X_extra_combined])\n",
    "        else:\n",
    "            X_final = X_pca\n",
    "            \n",
    "        return self.model_.predict(X_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e4da77-9d28-4739-bdaf-e532e590df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputSVRWithPCA:\n",
    "    \"\"\"\n",
    "    A class for multi-output Support Vector Regression with PCA preprocessing\n",
    "    and hyperparameter tuning using GridSearchCV.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        \"\"\"Initialize the class with empty attributes.\"\"\"\n",
    "        self.features_df = None\n",
    "        self.labels_df = None\n",
    "        self.extra_features_df = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None  # Added train/test split attributes\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.label_columns = ['TSV', 'B', 'SR', 'S', 'U', 'O']\n",
    "        self.logistic_model = \"4pl\"\n",
    "        self.extra_features_option = None  # Options: 'none', 'niqe', 'ssim', 'niqe+ssim'\n",
    "        self.results_comparison = {}  # Store results for different combinations\n",
    "        self.random_state = random_state  # Single random state for the entire pipeline\n",
    "        \n",
    "    def load_data(self, features_file, labels_file, extra_features_file=None):\n",
    "        \"\"\"Load features and labels from CSV files, including extra features.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        # Load features file\n",
    "        self.features_df = pd.read_csv(features_file)\n",
    "        print(f\"Features shape: {self.features_df.shape}\")\n",
    "        # Load labels file\n",
    "        self.labels_df = pd.read_csv(labels_file)\n",
    "        print(f\"Labels shape: {self.labels_df.shape}\")\n",
    "        \n",
    "        # Load extra features file\n",
    "        if extra_features_file is not None:\n",
    "            self.extra_features_df = pd.read_csv(extra_features_file)\n",
    "            print(f\"Extra features shape: {self.extra_features_df.shape}\")\n",
    "        \n",
    "        # Merge dataframes on videoname\n",
    "        merged_df = pd.merge(self.features_df, self.labels_df, on='videoname')\n",
    "        print(f\"Merged data shape: {merged_df.shape}\")\n",
    "        \n",
    "        if extra_features_file is not None:\n",
    "            # Merge with extra features\n",
    "            merged_df = pd.merge(merged_df, self.extra_features_df[['videoname', 'MeanNIQE', 'MeanSSIM']], on='videoname')\n",
    "            print(f\"Merged with extra features shape: {merged_df.shape}\")\n",
    "            \n",
    "            # Check for missing values in extra features\n",
    "            missing_niqe = merged_df['MeanNIQE'].isna().sum()\n",
    "            missing_ssim = merged_df['MeanSSIM'].isna().sum()\n",
    "            if missing_niqe > 0 or missing_ssim > 0:\n",
    "                print(f\"WARNING: Missing values - MeanNIQE: {missing_niqe}, MeanSSIM: {missing_ssim}\")\n",
    "                # Fill missing values with median\n",
    "                merged_df['MeanNIQE'].fillna(merged_df['MeanNIQE'].median(), inplace=True)\n",
    "                merged_df['MeanSSIM'].fillna(merged_df['MeanSSIM'].median(), inplace=True)\n",
    "                print(\"Missing values filled with median.\")\n",
    "        \n",
    "        # Extract features (all columns except videoname and labels)\n",
    "        feature_columns = [col for col in self.features_df.columns if col != 'videoname']\n",
    "        X_main = merged_df[feature_columns].values\n",
    "        \n",
    "        # Always add extra features columns (MeanNIQE, MeanSSIM) as last 2 columns\n",
    "        if extra_features_file is not None:\n",
    "            X_extra = merged_df[['MeanNIQE', 'MeanSSIM']].values\n",
    "            self.X = np.hstack([X_main, X_extra])\n",
    "        else:\n",
    "            # If no extra features, add dummy columns with zeros\n",
    "            X_extra = np.zeros((X_main.shape[0], 2))\n",
    "            self.X = np.hstack([X_main, X_extra])\n",
    "            print(\"WARNING: No extra features file provided. Using zeros for MeanNIQE and MeanSSIM.\")\n",
    "        \n",
    "        # Extract labels\n",
    "        self.y = merged_df[self.label_columns].values\n",
    "        print(f\"Final features shape: {self.X.shape}\")\n",
    "        print(f\"Final labels shape: {self.y.shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "        \n",
    "    def split_data(self, test_size=0.2, random_state=None):\n",
    "        \"\"\"Split data into train and test sets.\"\"\"\n",
    "        random_state = random_state or self.random_state\n",
    "        print(f\"\\nSplitting data into train/test sets (test_size={test_size}) with random_state={random_state}...\")\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Test data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "    \n",
    "    def _logistic_4pl(self, x, A, D, C, B):\n",
    "        \"\"\"4-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / (1 + (x / C) ** B)\n",
    "    \n",
    "    def _logistic_5pl(self, x, A, D, C, B, G):\n",
    "        \"\"\"5-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / ((1 + (x / C) ** B) ** G)\n",
    "        \n",
    "    def _logistic_fit_and_map(self, y_pred: np.ndarray, y_true: np.ndarray, model: str = None):\n",
    "        \"\"\"\n",
    "        Fit 4PL/5PL logistic function and return mapped predictions and metrics.\n",
    "        \"\"\"\n",
    "        model = (model or self.logistic_model).lower()\n",
    "        x = np.asarray(y_pred).ravel()\n",
    "        y = np.asarray(y_true).ravel()\n",
    "        if model == \"4pl\":\n",
    "            func = self._logistic_4pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0]\n",
    "        else:\n",
    "            func = self._logistic_5pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0, 1.0]\n",
    "        popt, _ = curve_fit(func, x, y, p0=beta0, maxfev=20000)\n",
    "        z = func(x, *popt)\n",
    "        plcc_fitted, _ = pearsonr(z, y)\n",
    "        spearman_fitted, _ = spearmanr(z, y)\n",
    "        rmse_fitted = float(np.sqrt(np.mean((z - y) ** 2)))\n",
    "        return z, popt, plcc_fitted, spearman_fitted, rmse_fitted\n",
    "        \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate performance metrics.\"\"\"\n",
    "        # Remove NaN values\n",
    "        mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "        y_true_clean = y_true[mask]\n",
    "        y_pred_clean = y_pred[mask]\n",
    "        \n",
    "        if len(y_true_clean) == 0:\n",
    "            return {'PLCC': np.nan, 'SRCC': np.nan, 'KRCC': np.nan, 'RMSE': np.nan, 'logistic_params': None}\n",
    "        \n",
    "        # PLCC with fitted predictions (4PL/5PL)\n",
    "        logistic_params = None\n",
    "        try:\n",
    "            _, params, plcc_fitted, _, _ = self._logistic_fit_and_map(\n",
    "                y_pred_clean, y_true_clean, model=self.logistic_model\n",
    "            )\n",
    "            plcc = plcc_fitted\n",
    "            logistic_params = params\n",
    "        except Exception as e:\n",
    "            print(f\"        Warning: Logistic fitting failed ({e}), using original predictions for PLCC\")\n",
    "            plcc, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "        \n",
    "        # Other metrics with original predictions\n",
    "        srcc, _ = spearmanr(y_true_clean, y_pred_clean)\n",
    "        krcc, _ = kendalltau(y_true_clean, y_pred_clean)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "        \n",
    "        return {\n",
    "            'PLCC': plcc,\n",
    "            'SRCC': srcc,\n",
    "            'KRCC': krcc,\n",
    "            'RMSE': rmse,\n",
    "            'logistic_params': logistic_params\n",
    "        }\n",
    "        \n",
    "    def tune_hyperparameters(self, cv=5, extra_features_option='niqe+ssim'):\n",
    "        \"\"\"Perform hyperparameter tuning using GridSearchCV on training data only.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not split yet. Run split_data() first.\")\n",
    "            \n",
    "        print(f\"\\nPerforming hyperparameter tuning with {cv}-fold cross-validation...\")\n",
    "        print(f\"Extra features configuration: {extra_features_option}\")\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            # 'variance_threshold': [0.8, 0.85, 0.9, 0.95, 0.975, 0.99, 1],\n",
    "            'variance_threshold': [0.99],\n",
    "            # 'kernel': ['linear', 'rbf'],\n",
    "            'kernel': ['rbf'],\n",
    "            # 'C': [0.1, 1, 10, 100],\n",
    "            'C': [10],\n",
    "            # 'gamma': ['scale', 'auto'],\n",
    "            'gamma': ['scale'],\n",
    "            # 'max_iter': [100, 500, -1]\n",
    "            'max_iter': [500]\n",
    "        }\n",
    "        \n",
    "        print(f\"Parameter grid size: {len(param_grid['variance_threshold']) * len(param_grid['kernel']) * len(param_grid['C']) * len(param_grid['gamma']) * len(param_grid['max_iter'])} combinations\")\n",
    "        \n",
    "        # Map the extra_features_option to indices\n",
    "        option_map = {\n",
    "            'none': [],\n",
    "            'niqe': [0],          # Only MeanNIQE\n",
    "            'ssim': [1],          # Only MeanSSIM\n",
    "            'niqe+ssim': [0, 1]   # Both MeanNIQE and MeanSSIM\n",
    "        }\n",
    "        \n",
    "        self.extra_features_option = extra_features_option\n",
    "        extra_indices = option_map.get(extra_features_option.lower(), [0, 1])\n",
    "        \n",
    "        # Perform grid search ONLY on training data - pass random_state to SVRWithVarianceThreshold\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=SVRWithVarianceThreshold(extra_features_to_use=extra_indices, random_state=self.random_state),\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"Starting grid search on training data...\")\n",
    "        grid_search.fit(self.X_train, self.y_train)  # Only training data\n",
    "        \n",
    "        # Store best parameters and model\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.model = grid_search.best_estimator_\n",
    "        print(\"Hyperparameter tuning completed!\")\n",
    "        \n",
    "        self.print_best_parameters()\n",
    "        \n",
    "    def print_best_parameters(self):\n",
    "        \"\"\"Print the best hyperparameters found by GridSearchCV.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "        print(\"=\"*50)\n",
    "        if self.best_params:\n",
    "            for param, value in self.best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "        else:\n",
    "            print(\"No hyperparameters found. Run tune_hyperparameters() first.\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the trained model on test data (default) or training data.\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"No trained model found. Run tune_hyperparameters() first.\")\n",
    "            return None\n",
    "            \n",
    "        if self.X_test is None:\n",
    "            raise ValueError(\"Test data not available. Run split_data() first.\")\n",
    "        X_eval, y_eval = self.X_test, self.y_test\n",
    "        dataset_name = \"TEST\"\n",
    "            \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"MODEL PERFORMANCE ON {dataset_name} DATA:\")\n",
    "        print(f\"Configuration: PCA + {self.extra_features_option}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_eval)\n",
    "        \n",
    "        # Calculate metrics for each output\n",
    "        print(f\"{'Label':<8} {'PLCC':<8} {'SRCC':<8} {'KRCC':<8} {'RMSE':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        overall_plcc = []\n",
    "        overall_srcc = []\n",
    "        overall_krcc = []\n",
    "        overall_rmse = []\n",
    "        \n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            metrics = self._calculate_metrics(y_eval[:, i], y_pred[:, i])\n",
    "            \n",
    "            print(f\"{label:<8} {metrics['PLCC']:<8.4f} {metrics['SRCC']:<8.4f} \"\n",
    "                  f\"{metrics['KRCC']:<8.4f} {metrics['RMSE']:<8.4f}\")\n",
    "            \n",
    "            # Store for overall calculations\n",
    "            if not np.isnan(metrics['PLCC']): overall_plcc.append(metrics['PLCC'])\n",
    "            if not np.isnan(metrics['SRCC']): overall_srcc.append(metrics['SRCC'])\n",
    "            if not np.isnan(metrics['KRCC']): overall_krcc.append(metrics['KRCC'])\n",
    "            if not np.isnan(metrics['RMSE']): overall_rmse.append(metrics['RMSE'])\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall metrics across all outputs\n",
    "        mean_metrics = {}\n",
    "        if overall_plcc:\n",
    "            mean_metrics = {\n",
    "                'PLCC': np.mean(overall_plcc),\n",
    "                'SRCC': np.mean(overall_srcc),\n",
    "                'KRCC': np.mean(overall_krcc),\n",
    "                'RMSE': np.mean(overall_rmse)\n",
    "            }\n",
    "            print(f\"Mean     {mean_metrics['PLCC']:<8.4f} {mean_metrics['SRCC']:<8.4f} \"\n",
    "                  f\"{mean_metrics['KRCC']:<8.4f} {mean_metrics['RMSE']:<8.4f}\")\n",
    "        \n",
    "        return mean_metrics\n",
    "    \n",
    "    def compare_all_combinations(self, features_file, labels_file, extra_features_file=None, \n",
    "                                cv=5, test_size=0.2, logistic_model=\"4pl\"):\n",
    "        \"\"\"Train and evaluate all feature combinations: none, niqe, ssim, niqe+ssim.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARING ALL FEATURE COMBINATIONS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        combinations = ['none', 'niqe', 'ssim', 'niqe+ssim']\n",
    "        \n",
    "        # Load data once\n",
    "        self.logistic_model = logistic_model\n",
    "        self.load_data(features_file, labels_file, extra_features_file)\n",
    "        self.split_data(test_size=test_size, random_state=self.random_state)\n",
    "        \n",
    "        for combo in combinations:\n",
    "            print(f\"\\n{'='*20} TRAINING WITH {combo.upper()} {'='*20}\")\n",
    "            \n",
    "            # Train model with current combination\n",
    "            self.tune_hyperparameters(cv=cv, extra_features_option=combo)\n",
    "            \n",
    "            # Evaluate and store results\n",
    "            metrics = self.evaluate_model()\n",
    "            if metrics:\n",
    "                self.results_comparison[combo] = metrics\n",
    "        \n",
    "        # Print comparison summary\n",
    "        self.print_comparison_summary()\n",
    "    \n",
    "    def print_comparison_summary(self):\n",
    "        \"\"\"Print comparison summary across all feature combinations.\"\"\"\n",
    "        if not self.results_comparison:\n",
    "            print(\"No results to compare. Run compare_all_combinations() first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"FEATURE COMBINATION COMPARISON SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Combination':<15} {'PLCC':<8} {'SRCC':<8} {'KRCC':<8} {'RMSE':<8}\")\n",
    "        print(\"-\" * 55)\n",
    "        \n",
    "        # Sort by PLCC (descending)\n",
    "        sorted_results = sorted(self.results_comparison.items(), \n",
    "                              key=lambda x: x[1]['PLCC'], reverse=True)\n",
    "        \n",
    "        for combo, metrics in sorted_results:\n",
    "            print(f\"{combo:<15} {metrics['PLCC']:<8.4f} {metrics['SRCC']:<8.4f} \"\n",
    "                  f\"{metrics['KRCC']:<8.4f} {metrics['RMSE']:<8.4f}\")\n",
    "        \n",
    "        print(\"-\" * 55)\n",
    "        best_combo = sorted_results[0][0]\n",
    "        print(f\"Best combination: {best_combo} (PLCC: {sorted_results[0][1]['PLCC']:.4f})\")\n",
    "    \n",
    "    def save_model(self, filepath=None):\n",
    "        \"\"\"Save the complete trained model pipeline to a pickle file.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Run the complete pipeline first.\")\n",
    "        \n",
    "        if filepath is None:\n",
    "            filepath = f\"svr_pca_model.pkl\"\n",
    "        \n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.model,\n",
    "            'best_params': self.best_params,\n",
    "            'label_columns': self.label_columns,\n",
    "            'logistic_model': self.logistic_model,\n",
    "            'pca_n_components': self.model.pca_.n_components_ if hasattr(self.model, 'pca_') else None,\n",
    "            'explained_variance_ratio': self.model.pca_.explained_variance_ratio_ if hasattr(self.model, 'pca_') else None,\n",
    "            'save_timestamp': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_package, f)\n",
    "            \n",
    "            print(f\"\\nModel saved successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            print(f\"File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a previously saved model from a pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_package = pickle.load(f)\n",
    "            \n",
    "            self.model = model_package['model']\n",
    "            self.best_params = model_package['best_params']\n",
    "            self.label_columns = model_package['label_columns']\n",
    "            self.logistic_model = model_package.get('logistic_model', '4pl')\n",
    "            \n",
    "            print(f\"\\nModel loaded successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            \n",
    "            if model_package.get('pca_n_components'):\n",
    "                print(f\"PCA components: {model_package['pca_n_components']}\")\n",
    "            print(f\"Logistic model: {self.logistic_model}\")\n",
    "            print(f\"Saved on: {model_package.get('save_timestamp', 'Unknown')}\")\n",
    "            \n",
    "            if self.best_params:\n",
    "                print(\"\\nLoaded hyperparameters:\")\n",
    "                for param, value in self.best_params.items():\n",
    "                    print(f\"  {param}: {value}\")\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def run_complete_pipeline(\n",
    "        self,\n",
    "        features_file=None,\n",
    "        labels_file=None,\n",
    "        extra_features_file=None,\n",
    "        cv=5,\n",
    "        test_size=0.2,\n",
    "        save_model_path=None,\n",
    "        logistic_model=\"4pl\",\n",
    "        feature_combination='niqe+ssim',  # Options: 'none', 'niqe', 'ssim', 'niqe+ssim'\n",
    "        random_state=42  # Accept random_state parameter\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the complete SVR pipeline with specified feature combination.\n",
    "        \n",
    "        Parameters:\n",
    "        feature_combination (str): Choose which features to use:\n",
    "            - 'none': PCA components only\n",
    "            - 'niqe': PCA components + MeanNIQE\n",
    "            - 'ssim': PCA components + MeanSSIM  \n",
    "            - 'niqe+ssim': PCA components + both MeanNIQE and MeanSSIM\n",
    "        random_state (int): Random state for reproducible results\n",
    "        \"\"\"\n",
    "        # Set the random state for the entire pipeline\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        print(\"Running SVR with PCA pipeline...\")\n",
    "        print(f\"Features: {features_file}\")\n",
    "        print(f\"Labels:   {labels_file}\")\n",
    "        print(f\"Extra features: {extra_features_file}\")\n",
    "        print(f\"Feature combination: {feature_combination}\")\n",
    "        print(f\"Logistic model: {logistic_model}\")\n",
    "        print(f\"Random state: {random_state}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.logistic_model = logistic_model\n",
    "        self.load_data(features_file, labels_file, extra_features_file)\n",
    "        self.split_data(test_size=test_size, random_state=self.random_state)  # Split data first\n",
    "        self.tune_hyperparameters(cv, extra_features_option=feature_combination)  # Train only on training data\n",
    "        \n",
    "        # Evaluate on test data for realistic performance estimate\n",
    "        mean_metrics = self.evaluate_model()\n",
    "        \n",
    "        # Save model after training\n",
    "        if save_model_path:\n",
    "            self.save_model(save_model_path)\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        return mean_metrics\n",
    "        \n",
    "    def predict(self, new_features):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Run the complete pipeline first or load a saved model.\")\n",
    "        return self.model.predict(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7253e4e-444a-485e-bb91-4b5556faa3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVR with PCA pipeline...\n",
      "Features: .\\dataset\\cleaned-svd-features.csv\n",
      "Labels:   .\\dataset\\cleaned-mos.csv\n",
      "Extra features: .\\dataset\\cleaned-extra-features.csv\n",
      "Feature combination: niqe+ssim\n",
      "Logistic model: 4pl\n",
      "Random state: 42\n",
      "============================================================\n",
      "Loading data...\n",
      "Features shape: (1000, 1153)\n",
      "Labels shape: (1000, 7)\n",
      "Extra features shape: (1000, 3)\n",
      "Merged data shape: (1000, 1159)\n",
      "Merged with extra features shape: (1000, 1161)\n",
      "Final features shape: (1000, 1154)\n",
      "Final labels shape: (1000, 6)\n",
      "Data loading completed successfully!\n",
      "\n",
      "Splitting data into train/test sets (test_size=0.2) with random_state=42...\n",
      "Training data shape: X=(800, 1154), y=(800, 6)\n",
      "Test data shape: X=(200, 1154), y=(200, 6)\n",
      "\n",
      "Performing hyperparameter tuning with 5-fold cross-validation...\n",
      "Extra features configuration: niqe+ssim\n",
      "Parameter grid size: 1 combinations\n",
      "Starting grid search on training data...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Hyperparameter tuning completed!\n",
      "\n",
      "==================================================\n",
      "BEST HYPERPARAMETERS FOUND:\n",
      "==================================================\n",
      "C: 10\n",
      "gamma: scale\n",
      "kernel: rbf\n",
      "max_iter: 500\n",
      "variance_threshold: 0.99\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ON TEST DATA:\n",
      "Configuration: PCA + niqe+ssim\n",
      "============================================================\n",
      "Label    PLCC     SRCC     KRCC     RMSE    \n",
      "--------------------------------------------------\n",
      "TSV      0.8748   0.8506   0.6654   0.3220  \n",
      "B        0.7391   0.7171   0.5252   0.5554  \n",
      "SR       0.7552   0.7541   0.5638   0.4510  \n",
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "S        0.8865   0.8801   0.6918   0.3668  \n",
      "U        0.8629   0.8599   0.6668   0.4058  \n",
      "O        0.8825   0.8751   0.6883   0.5131  \n",
      "--------------------------------------------------\n",
      "Mean     0.8335   0.8228   0.6336   0.4357  \n",
      "\n",
      "Model saved successfully!\n",
      "Filepath: D:\\Vishal\\Studies\\Sem_8\\FYP\\no-reference-colonoscopic-vqa\\objective-1\\regressors-4-niqe-ssim\\svr-pca\\trained_models\\svr_pca_model.pkl\n",
      "File size: 12.93 MB\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Pipeline execution finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    svr_model = MultiOutputSVRWithPCA()\n",
    "    svr_model.run_complete_pipeline(\n",
    "        features_file = r\".\\dataset\\cleaned-svd-features.csv\", \n",
    "        labels_file = r\".\\dataset\\cleaned-mos.csv\",\n",
    "        extra_features_file= r\".\\dataset\\cleaned-extra-features.csv\",\n",
    "        feature_combination=\"niqe+ssim\",\n",
    "        save_model_path = r\".\\trained_models\\svr_pca_model.pkl\",\n",
    "        test_size = 0.2,\n",
    "        cv = 2,\n",
    "        random_state = 42\n",
    "    )\n",
    "    print(\"Pipeline execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74cb7a7-b206-4638-97ba-be88988c4bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
