{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00aaa2a5-969f-41f6-b0e6-ef7e6f62d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "# Suppress warnings globally\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a813ebf7-5459-47b6-b643-d81361eb43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRWithVarianceThreshold(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Custom SVR estimator with integrated PCA preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, variance_threshold=0.95, kernel='rbf', C=1, \n",
    "                 gamma='scale', epsilon=0.1, max_iter=-1):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Apply power scaling and PCA with the specified variance threshold\n",
    "        self.scaler_ = PowerTransformer()\n",
    "        X_scaled = self.scaler_.fit_transform(X)\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca.fit(X_scaled)\n",
    "        cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum_variance >= self.variance_threshold) + 1\n",
    "        self.pca_ = PCA(n_components=n_components)\n",
    "        X_pca = self.pca_.fit_transform(X_scaled)\n",
    "        \n",
    "        # **FIX: Store the sign correction for consistency**\n",
    "        self.pca_signs_ = np.sign(self.pca_.components_[0, :n_components])\n",
    "        X_pca *= self.pca_signs_  # Apply sign correction\n",
    "        \n",
    "        # Create and fit the SVR model\n",
    "        base_svr = SVR(kernel=self.kernel, C=self.C, \n",
    "                      gamma=self.gamma, epsilon=self.epsilon, \n",
    "                      max_iter=self.max_iter)\n",
    "        self.model_ = MultiOutputRegressor(base_svr)\n",
    "        self.model_.fit(X_pca, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        X_pca = self.pca_.transform(X_scaled)\n",
    "        X_pca *= self.pca_signs_  # **FIX: Apply same sign correction**\n",
    "        return self.model_.predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c87b3111-f29e-4e98-9042-b35fd396b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputSVRWithPCA:\n",
    "    \"\"\"\n",
    "    A class for multi-output Support Vector Regression with PCA preprocessing\n",
    "    and hyperparameter tuning using GridSearchCV.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the class with empty attributes.\"\"\"\n",
    "        self.features_df = None\n",
    "        self.labels_df = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None  # Added train/test split attributes\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.label_columns = ['TSV', 'B', 'SR', 'S', 'U', 'O']\n",
    "        self.logistic_model = \"4pl\"\n",
    "        \n",
    "    def load_data(self, features_file, labels_file):\n",
    "        \"\"\"Load features and labels from CSV files.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        # Load features file\n",
    "        self.features_df = pd.read_csv(features_file)\n",
    "        print(f\"Features shape: {self.features_df.shape}\")\n",
    "        # Load labels file\n",
    "        self.labels_df = pd.read_csv(labels_file)\n",
    "        print(f\"Labels shape: {self.labels_df.shape}\")\n",
    "        # Merge dataframes on videoname\n",
    "        merged_df = pd.merge(self.features_df, self.labels_df, on='videoname')\n",
    "        print(f\"Merged data shape: {merged_df.shape}\")\n",
    "        # Extract features (all columns except videoname)\n",
    "        feature_columns = [col for col in self.features_df.columns if col != 'videoname']\n",
    "        self.X = merged_df[feature_columns].values\n",
    "        # Extract labels\n",
    "        self.y = merged_df[self.label_columns].values\n",
    "        print(f\"Final features shape: {self.X.shape}\")\n",
    "        print(f\"Final labels shape: {self.y.shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Split data into train and test sets.\"\"\"\n",
    "        print(f\"\\nSplitting data into train/test sets (test_size={test_size})...\")\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Test data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "\n",
    "    # Removed preprocess_features method since PCA is now handled inside SVRWithVarianceThreshold\n",
    "    \n",
    "    def _logistic_4pl(self, x, A, D, C, B):\n",
    "        \"\"\"4-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / (1 + (x / C) ** B)\n",
    "    \n",
    "    def _logistic_5pl(self, x, A, D, C, B, G):\n",
    "        \"\"\"5-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / ((1 + (x / C) ** B) ** G)\n",
    "        \n",
    "    def _logistic_fit_and_map(self, y_pred: np.ndarray, y_true: np.ndarray, model: str = None):\n",
    "        \"\"\"\n",
    "        Fit 4PL/5PL logistic function and return mapped predictions and metrics.\n",
    "        \"\"\"\n",
    "        model = (model or self.logistic_model).lower()\n",
    "        x = np.asarray(y_pred).ravel()\n",
    "        y = np.asarray(y_true).ravel()\n",
    "        if model == \"4pl\":\n",
    "            func = self._logistic_4pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0]\n",
    "        else:\n",
    "            func = self._logistic_5pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0, 1.0]\n",
    "        popt, _ = curve_fit(func, x, y, p0=beta0, maxfev=20000)\n",
    "        z = func(x, *popt)\n",
    "        plcc_fitted, _ = pearsonr(z, y)\n",
    "        spearman_fitted, _ = spearmanr(z, y)\n",
    "        rmse_fitted = float(np.sqrt(np.mean((z - y) ** 2)))\n",
    "        return z, popt, plcc_fitted, spearman_fitted, rmse_fitted\n",
    "        \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate performance metrics.\"\"\"\n",
    "        # Remove NaN values\n",
    "        mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "        y_true_clean = y_true[mask]\n",
    "        y_pred_clean = y_pred[mask]\n",
    "        \n",
    "        if len(y_true_clean) == 0:\n",
    "            return {'PLCC': np.nan, 'SRCC': np.nan, 'KRCC': np.nan, 'RMSE': np.nan, 'logistic_params': None}\n",
    "        \n",
    "        # PLCC with fitted predictions (4PL/5PL)\n",
    "        logistic_params = None\n",
    "        try:\n",
    "            _, params, plcc_fitted, _, _ = self._logistic_fit_and_map(\n",
    "                y_pred_clean, y_true_clean, model=self.logistic_model\n",
    "            )\n",
    "            plcc = plcc_fitted\n",
    "            logistic_params = params\n",
    "        except Exception as e:\n",
    "            print(f\"        Warning: Logistic fitting failed ({e}), using original predictions for PLCC\")\n",
    "            plcc, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "        \n",
    "        # Other metrics with original predictions\n",
    "        srcc, _ = spearmanr(y_true_clean, y_pred_clean)\n",
    "        krcc, _ = kendalltau(y_true_clean, y_pred_clean)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "        \n",
    "        return {\n",
    "            'PLCC': plcc,\n",
    "            'SRCC': srcc,\n",
    "            'KRCC': krcc,\n",
    "            'RMSE': rmse,\n",
    "            'logistic_params': logistic_params\n",
    "        }\n",
    "        \n",
    "    def tune_hyperparameters(self, cv=5):\n",
    "        \"\"\"Perform hyperparameter tuning using GridSearchCV on training data only.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not split yet. Run split_data() first.\")\n",
    "            \n",
    "        print(f\"\\nPerforming hyperparameter tuning with {cv}-fold cross-validation...\")\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            # 'variance_threshold': [0.8, 0.85, 0.9, 0.95, 0.975, 0.99, 1],\n",
    "            'variance_threshold': [0.99],\n",
    "            # 'kernel': ['linear', 'rbf'],\n",
    "            'kernel': ['rbf'],\n",
    "            # 'C': [0.1, 1, 10, 100],\n",
    "            'C': [10],\n",
    "            # 'gamma': ['scale', 'auto'],\n",
    "            'gamma': ['scale'],\n",
    "            # 'max_iter': [100, 500, -1]\n",
    "            'max_iter': [500]\n",
    "        }\n",
    "        \n",
    "        print(f\"Parameter grid size: {len(param_grid['variance_threshold']) * len(param_grid['kernel']) * len(param_grid['C']) * len(param_grid['gamma']) * len(param_grid['max_iter'])} combinations\")\n",
    "        \n",
    "        # Perform grid search ONLY on training data\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=SVRWithVarianceThreshold(),\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"Starting grid search on training data...\")\n",
    "        grid_search.fit(self.X_train, self.y_train)  # Only training data\n",
    "        \n",
    "        # Store best parameters and model\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.model = grid_search.best_estimator_\n",
    "        print(\"Hyperparameter tuning completed!\")\n",
    "        \n",
    "        self.print_best_parameters()\n",
    "        \n",
    "    def print_best_parameters(self):\n",
    "        \"\"\"Print the best hyperparameters found by GridSearchCV.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "        print(\"=\"*50)\n",
    "        if self.best_params:\n",
    "            for param, value in self.best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "        else:\n",
    "            print(\"No hyperparameters found. Run tune_hyperparameters() first.\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the trained model on test data (default) or training data.\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"No trained model found. Run tune_hyperparameters() first.\")\n",
    "            return\n",
    "            \n",
    "        if self.X_test is None:\n",
    "            raise ValueError(\"Test data not available. Run split_data() first.\")\n",
    "        X_eval, y_eval = self.X_test, self.y_test\n",
    "        dataset_name = \"TEST\"\n",
    "            \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"MODEL PERFORMANCE ON {dataset_name} DATA:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_eval)\n",
    "        \n",
    "        # Calculate metrics for each output\n",
    "        print(f\"{'Label':<8} {'PLCC':<8} {'SRCC':<8} {'KRCC':<8} {'RMSE':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        overall_plcc = []\n",
    "        overall_srcc = []\n",
    "        overall_krcc = []\n",
    "        overall_rmse = []\n",
    "        \n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            metrics = self._calculate_metrics(y_eval[:, i], y_pred[:, i])\n",
    "            \n",
    "            print(f\"{label:<8} {metrics['PLCC']:<8.4f} {metrics['SRCC']:<8.4f} \"\n",
    "                  f\"{metrics['KRCC']:<8.4f} {metrics['RMSE']:<8.4f}\")\n",
    "            \n",
    "            # Store for overall calculations\n",
    "            if not np.isnan(metrics['PLCC']): overall_plcc.append(metrics['PLCC'])\n",
    "            if not np.isnan(metrics['SRCC']): overall_srcc.append(metrics['SRCC'])\n",
    "            if not np.isnan(metrics['KRCC']): overall_krcc.append(metrics['KRCC'])\n",
    "            if not np.isnan(metrics['RMSE']): overall_rmse.append(metrics['RMSE'])\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall metrics across all outputs\n",
    "        if overall_plcc:\n",
    "            print(f\"Mean     {np.mean(overall_plcc):<8.4f} {np.mean(overall_srcc):<8.4f} \"\n",
    "                  f\"{np.mean(overall_krcc):<8.4f} {np.mean(overall_rmse):<8.4f}\")\n",
    "\n",
    "    def save_model(self, filepath=None):\n",
    "        \"\"\"Save the complete trained model pipeline to a pickle file.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Run the complete pipeline first.\")\n",
    "        \n",
    "        if filepath is None:\n",
    "            filepath = f\"svr_pca_model.pkl\"\n",
    "        \n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.model,\n",
    "            'best_params': self.best_params,\n",
    "            'label_columns': self.label_columns,\n",
    "            'logistic_model': self.logistic_model,\n",
    "            'pca_n_components': self.model.pca_.n_components_ if hasattr(self.model, 'pca_') else None,\n",
    "            'explained_variance_ratio': self.model.pca_.explained_variance_ratio_ if hasattr(self.model, 'pca_') else None,\n",
    "            'save_timestamp': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_package, f)\n",
    "            \n",
    "            print(f\"\\nModel saved successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            print(f\"File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a previously saved model from a pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_package = pickle.load(f)\n",
    "            \n",
    "            self.model = model_package['model']\n",
    "            self.best_params = model_package['best_params']\n",
    "            self.label_columns = model_package['label_columns']\n",
    "            self.logistic_model = model_package.get('logistic_model', '4pl')\n",
    "            \n",
    "            print(f\"\\nModel loaded successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            \n",
    "            if model_package.get('pca_n_components'):\n",
    "                print(f\"PCA components: {model_package['pca_n_components']}\")\n",
    "            print(f\"Logistic model: {self.logistic_model}\")\n",
    "            print(f\"Saved on: {model_package.get('save_timestamp', 'Unknown')}\")\n",
    "            \n",
    "            if self.best_params:\n",
    "                print(\"\\nLoaded hyperparameters:\")\n",
    "                for param, value in self.best_params.items():\n",
    "                    print(f\"  {param}: {value}\")\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def run_complete_pipeline(\n",
    "        self,\n",
    "        features_file=None,\n",
    "        labels_file=None,\n",
    "        cv=5,\n",
    "        test_size=0.2,\n",
    "        save_model_path=None,\n",
    "        logistic_model=\"4pl\"\n",
    "    ):\n",
    "        print(\"Running SVR with PCA pipeline...\")\n",
    "        print(f\"Features: {features_file}\")\n",
    "        print(f\"Labels:   {labels_file}\")\n",
    "        print(f\"Logistic model: {logistic_model}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.logistic_model = logistic_model\n",
    "        self.load_data(features_file, labels_file)\n",
    "        self.split_data(test_size=test_size, random_state=42)  # Split data first\n",
    "        self.tune_hyperparameters(cv)  # Train only on training data\n",
    "        \n",
    "        # Evaluate on test data for realistic performance estimate\n",
    "        self.evaluate_model()\n",
    "        \n",
    "        # Save model after training\n",
    "        if save_model_path:\n",
    "            self.save_model(save_model_path)\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        \n",
    "    def predict(self, new_features):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Run the complete pipeline first or load a saved model.\")\n",
    "        return self.model.predict(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7253e4e-444a-485e-bb91-4b5556faa3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SVR with PCA pipeline...\n",
      "Features: .\\dataset\\cleaned-svd-features.csv\n",
      "Labels:   .\\dataset\\cleaned-mos.csv\n",
      "Logistic model: 4pl\n",
      "============================================================\n",
      "Loading data...\n",
      "Features shape: (1000, 1153)\n",
      "Labels shape: (1000, 7)\n",
      "Merged data shape: (1000, 1159)\n",
      "Final features shape: (1000, 1152)\n",
      "Final labels shape: (1000, 6)\n",
      "Data loading completed successfully!\n",
      "\n",
      "Splitting data into train/test sets (test_size=0.2)...\n",
      "Training data shape: X=(800, 1152), y=(800, 6)\n",
      "Test data shape: X=(200, 1152), y=(200, 6)\n",
      "\n",
      "Performing hyperparameter tuning with 5-fold cross-validation...\n",
      "Parameter grid size: 3 combinations\n",
      "Starting grid search on training data...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Hyperparameter tuning completed!\n",
      "\n",
      "==================================================\n",
      "BEST HYPERPARAMETERS FOUND:\n",
      "==================================================\n",
      "C: 10\n",
      "gamma: scale\n",
      "kernel: rbf\n",
      "max_iter: 500\n",
      "variance_threshold: 0.99\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ON TEST DATA:\n",
      "============================================================\n",
      "Label    PLCC     SRCC     KRCC     RMSE    \n",
      "--------------------------------------------------\n",
      "TSV      0.8773   0.8527   0.6684   0.3186  \n",
      "B        0.7432   0.7212   0.5270   0.5518  \n",
      "SR       0.7575   0.7580   0.5618   0.4509  \n",
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "S        0.8871   0.8804   0.6879   0.3668  \n",
      "U        0.8573   0.8564   0.6621   0.4123  \n",
      "O        0.8826   0.8729   0.6846   0.5166  \n",
      "--------------------------------------------------\n",
      "Mean     0.8341   0.8236   0.6320   0.4362  \n",
      "\n",
      "Model saved successfully!\n",
      "Filepath: C:\\Users\\Dr. Priyanka Kokil_6\\Documents\\Colonoscopic VQA\\trained_models\\svr_pca_model.pkl\n",
      "File size: 13.15 MB\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Pipeline execution finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    svr_model = MultiOutputSVRWithPCA()\n",
    "    # Run pipeline with proper train/test split\n",
    "    svr_model.run_complete_pipeline(\n",
    "        features_file = r\".\\dataset\\cleaned-svd-features.csv\", \n",
    "        labels_file = r\".\\dataset\\cleaned-mos.csv\",\n",
    "        save_model_path = r\".\\trained_models\\svr_pca_model.pkl\",\n",
    "        test_size = 0.2,\n",
    "        cv = 1\n",
    "    )\n",
    "    print(\"Pipeline execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e5bb6-786e-4e94-88d0-82d1af7251bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
