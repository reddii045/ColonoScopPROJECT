{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a26f243-69b1-4fcb-9107-deaeba1e6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f91ca30-ad39-4bfe-9040-d6effa2ddde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankCORERegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, loss_type='plcc'):\n",
    "        super(RankCORERegressor, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def plcc_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate PLCC loss (maximize PLCC, so minimize negative PLCC)\"\"\"\n",
    "        # Ensure inputs are 1D\n",
    "        y_true = y_true.squeeze()\n",
    "        y_pred = y_pred.squeeze()\n",
    "        \n",
    "        # Center the data\n",
    "        y_true_mean = torch.mean(y_true)\n",
    "        y_pred_mean = torch.mean(y_pred)\n",
    "        y_true_centered = y_true - y_true_mean\n",
    "        y_pred_centered = y_pred - y_pred_mean\n",
    "        \n",
    "        # Calculate numerator and denominator\n",
    "        numerator = torch.sum(y_true_centered * y_pred_centered)\n",
    "        denominator = torch.sqrt(torch.sum(y_true_centered**2) * torch.sum(y_pred_centered**2))\n",
    "        \n",
    "        # Add a small epsilon to avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "        plcc_val = numerator / (denominator + epsilon)\n",
    "        \n",
    "        # We want to maximize PLCC, so our loss is the negative of PLCC\n",
    "        return -plcc_val\n",
    "\n",
    "    def srcc_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calculate SRCC loss (maximize SRCC, so minimize negative SRCC)\"\"\"\n",
    "        # Rank the true and predicted values\n",
    "        y_true_rank = torch.argsort(torch.argsort(y_true))\n",
    "        y_pred_rank = torch.argsort(torch.argsort(y_pred))\n",
    "        \n",
    "        # Calculate the squared differences between the ranks\n",
    "        d_squared = (y_true_rank.float() - y_pred_rank.float())**2\n",
    "        \n",
    "        # Number of elements\n",
    "        n = len(y_true)\n",
    "        \n",
    "        # Spearman's rank correlation coefficient formula\n",
    "        srcc_val = 1 - (6 * torch.sum(d_squared)) / (n * (n**2 - 1))\n",
    "        \n",
    "        # We want to maximize SRCC, so our loss is the negative of SRCC\n",
    "        return -srcc_val\n",
    "\n",
    "    def fit(self, X, y, n_epochs, learning_rate, batch_size):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            permutation = torch.randperm(X.size(0))\n",
    "            for i in range(0, X.size(0), batch_size):\n",
    "                indices = permutation[i:i + batch_size]\n",
    "                batch_X, batch_y = X[indices], y[indices]\n",
    "                \n",
    "                # Ensure batch is not too small for loss calculation\n",
    "                if len(batch_X) < 2:\n",
    "                    continue\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self.forward(batch_X)\n",
    "                if self.loss_type == 'plcc':\n",
    "                    loss = self.plcc_loss(batch_y, y_pred)\n",
    "                elif self.loss_type == 'srcc':\n",
    "                    loss = self.srcc_loss(batch_y, y_pred)\n",
    "                else:\n",
    "                    raise ValueError(\"loss_type must be 'plcc' or 'srcc'\")\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.forward(X_tensor)\n",
    "        return y_pred.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37adb504-b294-4a56-80fe-03ef91f8099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankCOREWithPCA(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Custom RankCORE estimator with integrated PCA preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, variance_threshold=0.99, n_epochs=100, learning_rate=0.01, \n",
    "                 loss_type='plcc', hidden_size=128, batch_size=16):\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_type = loss_type\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Apply power scaling and PCA with the specified variance threshold\n",
    "        self.scaler_ = PowerTransformer()\n",
    "        X_scaled = self.scaler_.fit_transform(X)\n",
    "        \n",
    "        pca = PCA()\n",
    "        pca.fit(X_scaled)\n",
    "        cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum_variance >= self.variance_threshold) + 1\n",
    "        self.pca_ = PCA(n_components=n_components)\n",
    "        X_pca = self.pca_.fit_transform(X_scaled)\n",
    "        \n",
    "        # Store the sign correction for consistency - fixed indexing issue\n",
    "        self.pca_signs_ = np.sign(self.pca_.components_[:, 0])\n",
    "        X_pca *= self.pca_signs_\n",
    "        \n",
    "        # Create separate models for each output\n",
    "        self.models_ = []\n",
    "        n_outputs = y.shape[1] if len(y.shape) > 1 else 1\n",
    "        \n",
    "        if n_outputs == 1:\n",
    "            # Single output case\n",
    "            model = RankCORERegressor(X_pca.shape[1], self.hidden_size, self.loss_type)\n",
    "            model.fit(X_pca, y, self.n_epochs, self.learning_rate, self.batch_size)\n",
    "            self.models_.append(model)\n",
    "        else:\n",
    "            # Multi-output case\n",
    "            for i in range(n_outputs):\n",
    "                model = RankCORERegressor(X_pca.shape[1], self.hidden_size, self.loss_type)\n",
    "                model.fit(X_pca, y[:, i], self.n_epochs, self.learning_rate, self.batch_size)\n",
    "                self.models_.append(model)\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        X_pca = self.pca_.transform(X_scaled)\n",
    "        X_pca *= self.pca_signs_\n",
    "        \n",
    "        if len(self.models_) == 1:\n",
    "            # Single output\n",
    "            return self.models_[0].predict(X_pca)\n",
    "        else:\n",
    "            # Multi-output\n",
    "            predictions = []\n",
    "            for model in self.models_:\n",
    "                pred = model.predict(X_pca)\n",
    "                predictions.append(pred)\n",
    "            return np.column_stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05880f97-8302-40e3-a5af-edeb88a8c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputRankCOREWithPCA:\n",
    "    \"\"\"\n",
    "    A class for multi-output RankCORE regression with PCA preprocessing\n",
    "    and hyperparameter tuning using GridSearchCV.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the class with empty attributes.\"\"\"\n",
    "        self.features_df = None\n",
    "        self.labels_df = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.label_columns = ['TSV', 'B', 'SR', 'S', 'U', 'O']\n",
    "        self.logistic_model = \"4pl\"\n",
    "        \n",
    "    def load_data(self, features_file, labels_file):\n",
    "        \"\"\"Load features and labels from CSV files.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        # Load features file\n",
    "        self.features_df = pd.read_csv(features_file)\n",
    "        print(f\"Features shape: {self.features_df.shape}\")\n",
    "        \n",
    "        # Load labels file\n",
    "        self.labels_df = pd.read_csv(labels_file)\n",
    "        print(f\"Labels shape: {self.labels_df.shape}\")\n",
    "        \n",
    "        # Merge dataframes on videoname\n",
    "        merged_df = pd.merge(self.features_df, self.labels_df, on='videoname')\n",
    "        print(f\"Merged data shape: {merged_df.shape}\")\n",
    "        \n",
    "        # Extract features (all columns except videoname)\n",
    "        feature_columns = [col for col in self.features_df.columns if col != 'videoname']\n",
    "        self.X = merged_df[feature_columns].values\n",
    "        \n",
    "        # Extract labels\n",
    "        self.y = merged_df[self.label_columns].values\n",
    "        \n",
    "        print(f\"Final features shape: {self.X.shape}\")\n",
    "        print(f\"Final labels shape: {self.y.shape}\")\n",
    "        print(\"Data loading completed successfully!\")\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Split data into train and test sets.\"\"\"\n",
    "        print(f\"\\nSplitting data into train/test sets (test_size={test_size})...\")\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        print(f\"Training data shape: X={self.X_train.shape}, y={self.y_train.shape}\")\n",
    "        print(f\"Test data shape: X={self.X_test.shape}, y={self.y_test.shape}\")\n",
    "    \n",
    "    def _logistic_4pl(self, x, A, D, C, B):\n",
    "        \"\"\"4-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / (1 + (x / C) ** B)\n",
    "    \n",
    "    def _logistic_5pl(self, x, A, D, C, B, G):\n",
    "        \"\"\"5-parameter logistic function.\"\"\"\n",
    "        return D + (A - D) / ((1 + (x / C) ** B) ** G)\n",
    "        \n",
    "    def _logistic_fit_and_map(self, y_pred: np.ndarray, y_true: np.ndarray, model: str = None):\n",
    "        \"\"\"\n",
    "        Fit 4PL/5PL logistic function and return mapped predictions and metrics.\n",
    "        \"\"\"\n",
    "        model = (model or self.logistic_model).lower()\n",
    "        x = np.asarray(y_pred).ravel()\n",
    "        y = np.asarray(y_true).ravel()\n",
    "        \n",
    "        if model == \"4pl\":\n",
    "            func = self._logistic_4pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0]\n",
    "        else:\n",
    "            func = self._logistic_5pl\n",
    "            beta0 = [float(np.max(y)), float(np.min(y)), float(np.median(x)), 1.0, 1.0]\n",
    "        \n",
    "        popt, _ = curve_fit(func, x, y, p0=beta0, maxfev=20000)\n",
    "        z = func(x, *popt)\n",
    "        plcc_fitted, _ = pearsonr(z, y)\n",
    "        spearman_fitted, _ = spearmanr(z, y)\n",
    "        rmse_fitted = float(np.sqrt(np.mean((z - y) ** 2)))\n",
    "        \n",
    "        return z, popt, plcc_fitted, spearman_fitted, rmse_fitted\n",
    "        \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate performance metrics.\"\"\"\n",
    "        # Remove NaN values\n",
    "        mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "        y_true_clean = y_true[mask]\n",
    "        y_pred_clean = y_pred[mask]\n",
    "        \n",
    "        if len(y_true_clean) == 0:\n",
    "            return {'PLCC': np.nan, 'SRCC': np.nan, 'KRCC': np.nan, 'RMSE': np.nan, 'logistic_params': None}\n",
    "        \n",
    "        # PLCC with fitted predictions (4PL/5PL)\n",
    "        logistic_params = None\n",
    "        try:\n",
    "            _, params, plcc_fitted, _, _ = self._logistic_fit_and_map(\n",
    "                y_pred_clean, y_true_clean, model=self.logistic_model\n",
    "            )\n",
    "            plcc = plcc_fitted\n",
    "            logistic_params = params\n",
    "        except Exception as e:\n",
    "            print(f\"        Warning: Logistic fitting failed ({e}), using original predictions for PLCC\")\n",
    "            plcc, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "        \n",
    "        # Other metrics with original predictions\n",
    "        srcc, _ = spearmanr(y_true_clean, y_pred_clean)\n",
    "        krcc, _ = kendalltau(y_true_clean, y_pred_clean)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "        \n",
    "        return {\n",
    "            'PLCC': plcc,\n",
    "            'SRCC': srcc,\n",
    "            'KRCC': krcc,\n",
    "            'RMSE': rmse,\n",
    "            'logistic_params': logistic_params\n",
    "        }\n",
    "        \n",
    "    def tune_hyperparameters(self, cv=5):\n",
    "        \"\"\"Perform hyperparameter tuning using GridSearchCV on training data only.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not split yet. Run split_data() first.\")\n",
    "            \n",
    "        print(f\"\\nPerforming hyperparameter tuning with {cv}-fold cross-validation...\")\n",
    "        \n",
    "        # Define parameter grid for RankCORE\n",
    "        param_grid = {\n",
    "            'variance_threshold': [0.99],  # PCA variance threshold\n",
    "            'n_epochs': [50, 100, 200],\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'loss_type': ['plcc', 'srcc'],\n",
    "            'hidden_size': [64, 128, 256],\n",
    "            'batch_size': [8, 16, 32]\n",
    "        }\n",
    "        \n",
    "        print(f\"Parameter grid size: {np.prod([len(v) for v in param_grid.values()])} combinations\")\n",
    "        \n",
    "        # Perform grid search ONLY on training data\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=RankCOREWithPCA(),\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=1,  # RankCORE uses PyTorch, better to use single job\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"Starting grid search on training data...\")\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # Store best parameters and model\n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.model = grid_search.best_estimator_\n",
    "        print(\"Hyperparameter tuning completed!\")\n",
    "        \n",
    "        self.print_best_parameters()\n",
    "        \n",
    "    def print_best_parameters(self):\n",
    "        \"\"\"Print the best hyperparameters found by GridSearchCV.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "        print(\"=\"*50)\n",
    "        if self.best_params:\n",
    "            for param, value in self.best_params.items():\n",
    "                print(f\"{param}: {value}\")\n",
    "        else:\n",
    "            print(\"No hyperparameters found. Run tune_hyperparameters() first.\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Evaluate the trained model on test data.\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"No trained model found. Run tune_hyperparameters() first.\")\n",
    "            return\n",
    "            \n",
    "        if self.X_test is None:\n",
    "            raise ValueError(\"Test data not available. Run split_data() first.\")\n",
    "            \n",
    "        X_eval, y_eval = self.X_test, self.y_test\n",
    "        dataset_name = \"TEST\"\n",
    "            \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"MODEL PERFORMANCE ON {dataset_name} DATA:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_eval)\n",
    "        \n",
    "        # Calculate metrics for each output\n",
    "        print(f\"{'Label':<8} {'PLCC':<8} {'SRCC':<8} {'KRCC':<8} {'RMSE':<8}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        overall_plcc = []\n",
    "        overall_srcc = []\n",
    "        overall_krcc = []\n",
    "        overall_rmse = []\n",
    "        \n",
    "        for i, label in enumerate(self.label_columns):\n",
    "            metrics = self._calculate_metrics(y_eval[:, i], y_pred[:, i])\n",
    "            \n",
    "            print(f\"{label:<8} {metrics['PLCC']:<8.4f} {metrics['SRCC']:<8.4f} \"\n",
    "                  f\"{metrics['KRCC']:<8.4f} {metrics['RMSE']:<8.4f}\")\n",
    "            \n",
    "            # Store for overall calculations\n",
    "            if not np.isnan(metrics['PLCC']): overall_plcc.append(metrics['PLCC'])\n",
    "            if not np.isnan(metrics['SRCC']): overall_srcc.append(metrics['SRCC'])\n",
    "            if not np.isnan(metrics['KRCC']): overall_krcc.append(metrics['KRCC'])\n",
    "            if not np.isnan(metrics['RMSE']): overall_rmse.append(metrics['RMSE'])\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall metrics across all outputs\n",
    "        if overall_plcc:\n",
    "            print(f\"Mean     {np.mean(overall_plcc):<8.4f} {np.mean(overall_srcc):<8.4f} \"\n",
    "                  f\"{np.mean(overall_krcc):<8.4f} {np.mean(overall_rmse):<8.4f}\")\n",
    "\n",
    "    def save_model(self, filepath=None):\n",
    "        \"\"\"Save the complete trained model pipeline to a pickle file.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Run the complete pipeline first.\")\n",
    "        \n",
    "        if filepath is None:\n",
    "            filepath = f\"rankcore_pca_model.pkl\"\n",
    "        \n",
    "        os.makedirs(os.path.dirname(filepath) if os.path.dirname(filepath) else '.', exist_ok=True)\n",
    "        \n",
    "        model_package = {\n",
    "            'model': self.model,\n",
    "            'best_params': self.best_params,\n",
    "            'label_columns': self.label_columns,\n",
    "            'logistic_model': self.logistic_model,\n",
    "            'pca_n_components': self.model.pca_.n_components_ if hasattr(self.model, 'pca_') else None,\n",
    "            'explained_variance_ratio': self.model.pca_.explained_variance_ratio_ if hasattr(self.model, 'pca_') else None,\n",
    "            'save_timestamp': datetime.datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_package, f)\n",
    "            \n",
    "            print(f\"\\nModel saved successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            print(f\"File size: {os.path.getsize(filepath) / (1024*1024):.2f} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a previously saved model from a pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_package = pickle.load(f)\n",
    "            \n",
    "            self.model = model_package['model']\n",
    "            self.best_params = model_package['best_params']\n",
    "            self.label_columns = model_package['label_columns']\n",
    "            self.logistic_model = model_package.get('logistic_model', '4pl')\n",
    "            \n",
    "            print(f\"\\nModel loaded successfully!\")\n",
    "            print(f\"Filepath: {os.path.abspath(filepath)}\")\n",
    "            \n",
    "            if model_package.get('pca_n_components'):\n",
    "                print(f\"PCA components: {model_package['pca_n_components']}\")\n",
    "            print(f\"Logistic model: {self.logistic_model}\")\n",
    "            print(f\"Saved on: {model_package.get('save_timestamp', 'Unknown')}\")\n",
    "            \n",
    "            if self.best_params:\n",
    "                print(\"\\nLoaded hyperparameters:\")\n",
    "                for param, value in self.best_params.items():\n",
    "                    print(f\"  {param}: {value}\")\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {filepath}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def run_complete_pipeline(\n",
    "        self,\n",
    "        features_file=None,\n",
    "        labels_file=None,\n",
    "        cv=5,\n",
    "        test_size=0.2,\n",
    "        save_model_path=None,\n",
    "        logistic_model=\"4pl\"\n",
    "    ):\n",
    "        print(\"Running RankCORE with PCA pipeline...\")\n",
    "        print(f\"Features: {features_file}\")\n",
    "        print(f\"Labels:   {labels_file}\")\n",
    "        print(f\"Logistic model: {logistic_model}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.logistic_model = logistic_model\n",
    "        self.load_data(features_file, labels_file)\n",
    "        self.split_data(test_size=test_size, random_state=42)\n",
    "        self.tune_hyperparameters(cv)\n",
    "        \n",
    "        # Evaluate on test data for realistic performance estimate\n",
    "        self.evaluate_model()\n",
    "        \n",
    "        # Save model after training\n",
    "        if save_model_path:\n",
    "            self.save_model(save_model_path)\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully!\")\n",
    "        \n",
    "    def predict(self, new_features):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Run the complete pipeline first or load a saved model.\")\n",
    "        return self.model.predict(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5ea26c-681f-4458-aa35-60c640e6a7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RankCORE with PCA pipeline...\n",
      "Features: .\\dataset\\cleaned-svd-features.csv\n",
      "Labels:   .\\dataset\\cleaned-mos.csv\n",
      "Logistic model: 4pl\n",
      "============================================================\n",
      "Loading data...\n",
      "Features shape: (1000, 1153)\n",
      "Labels shape: (1000, 7)\n",
      "Merged data shape: (1000, 1159)\n",
      "Final features shape: (1000, 1152)\n",
      "Final labels shape: (1000, 6)\n",
      "Data loading completed successfully!\n",
      "\n",
      "Splitting data into train/test sets (test_size=0.2)...\n",
      "Training data shape: X=(800, 1152), y=(800, 6)\n",
      "Test data shape: X=(200, 1152), y=(200, 6)\n",
      "\n",
      "Performing hyperparameter tuning with 3-fold cross-validation...\n",
      "Parameter grid size: 162 combinations\n",
      "Starting grid search on training data...\n",
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "243 fits failed out of a total of 486.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "243 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\857501402.py\", line 42, in fit\n",
      "    model.fit(X_pca, y[:, i], self.n_epochs, self.learning_rate, self.batch_size)\n",
      "  File \"C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\3025477235.py\", line 80, in fit\n",
      "    loss.backward()\n",
      "  File \"C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py\", line 647, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py\", line 354, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py\", line 829, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [-4.94160363e+00 -7.93342063e+00 -1.94463902e+01             nan\n",
      "             nan             nan -6.31414001e+02 -4.37893634e+03\n",
      " -3.85704020e+04             nan             nan             nan\n",
      " -1.29288421e+06 -1.12941811e+07 -1.45513393e+08             nan\n",
      "             nan             nan -6.05834082e+00 -1.27824627e+01\n",
      " -4.37173101e+01             nan             nan             nan\n",
      " -2.05855955e+03 -1.35752279e+04 -1.10234373e+05             nan\n",
      "             nan             nan -2.92210949e+06 -2.81975761e+07\n",
      " -3.75576270e+08             nan             nan             nan\n",
      " -9.11338903e+00 -2.58769677e+01 -1.08610274e+02             nan\n",
      "             nan             nan -4.87655790e+03 -3.58963743e+04\n",
      " -3.03139042e+05             nan             nan             nan\n",
      " -7.71386244e+06 -7.42509586e+07 -7.67835905e+08             nan\n",
      "             nan             nan -4.21963883e+00 -4.71390294e+00\n",
      " -6.63196516e+00             nan             nan             nan\n",
      " -1.00618850e+02 -4.28290418e+02 -2.83361456e+03             nan\n",
      "             nan             nan -2.08041232e+05 -6.73051684e+05\n",
      " -4.82824576e+06             nan             nan             nan\n",
      " -4.47127768e+00 -5.56137057e+00 -1.03562909e+01             nan\n",
      "             nan             nan -2.92002977e+02 -1.07561708e+03\n",
      " -7.68824648e+03             nan             nan             nan\n",
      " -3.41965575e+05 -1.28124627e+06 -1.19562389e+07             nan\n",
      "             nan             nan -5.38983675e+00 -7.67448667e+00\n",
      " -2.07149297e+01             nan             nan             nan\n",
      " -7.36306325e+02 -2.33194843e+03 -2.15286988e+04             nan\n",
      "             nan             nan -9.85174557e+05 -3.72947737e+06\n",
      " -2.66746625e+07             nan             nan             nan\n",
      " -4.35047824e+00 -4.41891345e+00 -4.46076161e+00             nan\n",
      "             nan             nan -2.86340003e+01 -6.09768862e+01\n",
      " -1.96650218e+02             nan             nan             nan\n",
      " -3.05237810e+04 -6.82468705e+04 -2.23764702e+05             nan\n",
      "             nan             nan -4.20406621e+00 -4.37287606e+00\n",
      " -5.36315446e+00             nan             nan             nan\n",
      " -6.50850188e+01 -1.24422019e+02 -4.91406553e+02             nan\n",
      "             nan             nan -9.37146612e+04 -1.79413057e+05\n",
      " -5.44526524e+05             nan             nan             nan\n",
      " -4.32775193e+00 -4.85152229e+00 -6.42399030e+00             nan\n",
      "             nan             nan -1.59947325e+02 -3.43398526e+02\n",
      " -1.13032778e+03             nan             nan             nan\n",
      " -2.99008071e+05 -4.54607969e+05 -1.33428831e+06             nan\n",
      "             nan             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning completed!\n",
      "\n",
      "==================================================\n",
      "BEST HYPERPARAMETERS FOUND:\n",
      "==================================================\n",
      "batch_size: 32\n",
      "hidden_size: 128\n",
      "learning_rate: 0.001\n",
      "loss_type: plcc\n",
      "n_epochs: 50\n",
      "variance_threshold: 0.99\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE ON TEST DATA:\n",
      "============================================================\n",
      "Label    PLCC     SRCC     KRCC     RMSE    \n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\1517468340.py:58: RuntimeWarning: invalid value encountered in power\n",
      "  return D + (A - D) / (1 + (x / C) ** B)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "TSV      0.8276   0.8190   0.6285   2.0956  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\1517468340.py:58: RuntimeWarning: invalid value encountered in power\n",
      "  return D + (A - D) / (1 + (x / C) ** B)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "B        0.6399   0.6107   0.4378   1.9569  \n",
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "SR       0.7215   0.7323   0.5393   1.4447  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\1517468340.py:58: RuntimeWarning: invalid value encountered in power\n",
      "  return D + (A - D) / (1 + (x / C) ** B)\n",
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\1517468340.py:58: RuntimeWarning: invalid value encountered in power\n",
      "  return D + (A - D) / (1 + (x / C) ** B)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "S        0.8353   0.8432   0.6516   1.6024  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\1517468340.py:58: RuntimeWarning: invalid value encountered in power\n",
      "  return D + (A - D) / (1 + (x / C) ** B)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "U        0.8036   0.8075   0.6055   1.6326  \n",
      "        Warning: Logistic fitting failed (Optimal parameters not found: Number of calls to function has reached maxfev = 20000.), using original predictions for PLCC\n",
      "O        0.8483   0.8459   0.6553   3.2639  \n",
      "--------------------------------------------------\n",
      "Mean     0.7794   0.7764   0.5863   1.9994  \n",
      "\n",
      "Model saved successfully!\n",
      "Filepath: C:\\Users\\Dr. Priyanka Kokil_6\\Documents\\Colonoscopic VQA\\rankcore-pca\\trained_models\\rankcore_pca_model.pkl\n",
      "File size: 4.66 MB\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Pipeline execution finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr. Priyanka Kokil_6\\AppData\\Local\\Temp\\ipykernel_16768\\1517468340.py:58: RuntimeWarning: invalid value encountered in power\n",
      "  return D + (A - D) / (1 + (x / C) ** B)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    rankcore_model = MultiOutputRankCOREWithPCA()\n",
    "    \n",
    "    # Run pipeline with proper train/test split - fixed cv value\n",
    "    rankcore_model.run_complete_pipeline(\n",
    "        features_file = r\".\\dataset\\cleaned-svd-features.csv\", \n",
    "        labels_file = r\".\\dataset\\cleaned-mos.csv\",\n",
    "        save_model_path = r\".\\trained_models\\rankcore_pca_model.pkl\",\n",
    "        test_size = 0.2,\n",
    "        cv = 3  # Fixed: Changed from cv=1 to cv=3 for proper cross-validation\n",
    "    )\n",
    "    print(\"Pipeline execution finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096cc42-f959-4093-aec2-54208a5abaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f69d7-3a1d-4f14-a4ac-24bb2b485b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
